"""
This type stub file was generated by pyright.
"""

import pyarrow as pa
from collections.abc import Iterable
from typing import Any, Optional, Union
from .features import Features
from .features.features import FeatureType

"""To write records into Parquet files."""
logger = ...
type_ = type
def get_arrow_writer_batch_size_from_features(features: Optional[Features]) -> Optional[int]:
    """
    Get the writer_batch_size that defines the maximum record batch size in the arrow files based on configuration values.
    The default value is 100 for image/audio datasets and 10 for videos.
    This allows to avoid overflows in arrow buffers.

    Args:
        features (`datasets.Features` or `None`):
            Dataset Features from `datasets`.
    Returns:
        writer_batch_size (`Optional[int]`):
            Writer batch size to pass to a dataset builder.
            If `None`, then it will use the `datasets` default, i.e. `datasets.config.DEFAULT_MAX_BATCH_SIZE`.
    """
    ...

def get_writer_batch_size_from_features(features: Optional[Features]) -> Optional[int]:
    """
    Get the writer_batch_size that defines the maximum row group size in the parquet files based on configuration values.
    By default these are not set, but it can be helpful to hard set those values in some cases.
    This allows to optimize random access to parquet file, since accessing 1 row requires
    to read its entire row group.

    Args:
        features (`datasets.Features` or `None`):
            Dataset Features from `datasets`.
    Returns:
        writer_batch_size (`Optional[int]`):
            Writer batch size to pass to a parquet writer.
            If `None`, then it will use the `datasets` default, i.e. aiming for row groups of 100MB.
    """
    ...

def get_writer_batch_size_from_data_size(num_rows: int, num_bytes: int) -> int:
    """
    Get the writer_batch_size that defines the maximum row group size in the parquet files.
    The default in `datasets` is aiming for row groups of maximum 100MB uncompressed.
    This allows to optimize random access to parquet file, since accessing 1 row requires
    to read its entire row group.

    This can be improved to get optimized size for querying/iterating
    but at least it matches the dataset viewer expectations on HF.

    Args:
        num_rows (`int`):
            Number of rows in the dataset.
        num_bytes (`int`):
            Number of bytes in the dataset.
            For dataset with external files to embed (image, audio, videos), this can also be an
            estimate from `dataset._estimate_nbytes()`.
    Returns:
        writer_batch_size (`Optional[int]`):
            Writer batch size to pass to a parquet writer.
    """
    ...

class SchemaInferenceError(ValueError):
    ...


class TypedSequence:
    """
    This data container generalizes the typing when instantiating pyarrow arrays, tables or batches.

    More specifically it adds several features:
    - Support extension types like ``datasets.features.Array2DExtensionType``:
        By default pyarrow arrays don't return extension arrays. One has to call
        ``pa.ExtensionArray.from_storage(type, pa.array(data, type.storage_type))``
        in order to get an extension array.
    - Support for ``try_type`` parameter that can be used instead of ``type``:
        When an array is transformed, we like to keep the same type as before if possible.
        For example when calling :func:`datasets.Dataset.map`, we don't want to change the type
        of each column by default.
    - Better error message when a pyarrow array overflows.

    Example::

        from datasets.features import Array2D, Array2DExtensionType, Value
        from datasets.arrow_writer import TypedSequence
        import pyarrow as pa

        arr = pa.array(TypedSequence([1, 2, 3], type=Value("int32")))
        assert arr.type == pa.int32()

        arr = pa.array(TypedSequence([1, 2, 3], try_type=Value("int32")))
        assert arr.type == pa.int32()

        arr = pa.array(TypedSequence(["foo", "bar"], try_type=Value("int32")))
        assert arr.type == pa.string()

        arr = pa.array(TypedSequence([[[1, 2, 3]]], type=Array2D((1, 3), "int64")))
        assert arr.type == Array2DExtensionType((1, 3), "int64")

        table = pa.Table.from_pydict({
            "image": TypedSequence([[[1, 2, 3]]], type=Array2D((1, 3), "int64"))
        })
        assert table["image"].type == Array2DExtensionType((1, 3), "int64")

    """
    def __init__(self, data: Iterable, type: Optional[FeatureType] = ..., try_type: Optional[FeatureType] = ..., optimized_int_type: Optional[FeatureType] = ...) -> None:
        ...
    
    def get_inferred_type(self) -> FeatureType:
        """Return the inferred feature type.
        This is done by converting the sequence to an Arrow array, and getting the corresponding
        feature type.

        Since building the Arrow array can be expensive, the value of the inferred type is cached
        as soon as pa.array is called on the typed sequence.

        Returns:
            FeatureType: inferred feature type of the sequence.
        """
        ...
    
    def __arrow_array__(self, type: Optional[pa.DataType] = ...):
        """This function is called when calling pa.array(typed_sequence)"""
        ...
    


class OptimizedTypedSequence(TypedSequence):
    def __init__(self, data, type: Optional[FeatureType] = ..., try_type: Optional[FeatureType] = ..., col: Optional[str] = ..., optimized_int_type: Optional[FeatureType] = ...) -> None:
        ...
    


class ArrowWriter:
    """Shuffles and writes Examples to Arrow files."""
    def __init__(self, schema: Optional[pa.Schema] = ..., features: Optional[Features] = ..., path: Optional[str] = ..., stream: Optional[pa.NativeFile] = ..., fingerprint: Optional[str] = ..., writer_batch_size: Optional[int] = ..., hash_salt: Optional[str] = ..., check_duplicates: Optional[bool] = ..., disable_nullable: bool = ..., update_features: bool = ..., with_metadata: bool = ..., unit: str = ..., embed_local_files: bool = ..., storage_options: Optional[dict] = ...) -> None:
        ...
    
    def __len__(self): # -> int:
        """Return the number of writed and staged examples"""
        ...
    
    def __enter__(self): # -> Self:
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    
    def close(self): # -> None:
        ...
    
    @property
    def schema(self): # -> list[Any]:
        ...
    
    def write_examples_on_file(self): # -> None:
        """Write stored examples from the write-pool of examples. It makes a table out of the examples and write it."""
        ...
    
    def write_rows_on_file(self): # -> None:
        """Write stored rows from the write-pool of rows. It concatenates the single-row tables and it writes the resulting table."""
        ...
    
    def write(self, example: dict[str, Any], key: Optional[Union[str, int, bytes]] = ..., writer_batch_size: Optional[int] = ...): # -> None:
        """Add a given (Example,Key) pair to the write-pool of examples which is written to file.

        Args:
            example: the Example to add.
            key: Optional, a unique identifier(str, int or bytes) associated with each example
        """
        ...
    
    def check_duplicate_keys(self): # -> None:
        """Raises error if duplicates found in a batch"""
        ...
    
    def write_row(self, row: pa.Table, writer_batch_size: Optional[int] = ...): # -> None:
        """Add a given single-row Table to the write-pool of rows which is written to file.

        Args:
            row: the row to add.
        """
        ...
    
    def write_batch(self, batch_examples: dict[str, list], writer_batch_size: Optional[int] = ..., try_original_type: Optional[bool] = ...): # -> None:
        """Write a batch of Example to file.
        Ignores the batch if it appears to be empty,
        preventing a potential schema update of unknown types.

        Args:
            batch_examples: the batch of examples to add.
            try_original_type: use `try_type` when instantiating OptimizedTypedSequence if `True`, otherwise `try_type = None`.
        """
        ...
    
    def write_table(self, pa_table: pa.Table, writer_batch_size: Optional[int] = ...): # -> None:
        """Write a Table to file.

        Args:
            example: the Table to add.
        """
        ...
    
    def finalize(self, close_stream=...): # -> tuple[int | Any, int | Any]:
        ...
    


class ParquetWriter(ArrowWriter):
    def __init__(self, *args, use_content_defined_chunking=..., write_page_index=..., **kwargs) -> None:
        ...
    


