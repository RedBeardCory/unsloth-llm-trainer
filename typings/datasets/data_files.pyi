"""
This type stub file was generated by pyright.
"""

import huggingface_hub
from typing import Optional, Union
from packaging import version
from . import config
from .download import DownloadConfig

SingleOriginMetadata = Union[tuple[str, str], tuple[str], tuple[()]]
SANITIZED_DEFAULT_SPLIT = ...
logger = ...
class Url(str):
    ...


class EmptyDatasetError(FileNotFoundError):
    ...


SPLIT_PATTERN_SHARDED = ...
SPLIT_KEYWORDS = ...
NON_WORDS_CHARS = ...
if config.FSSPEC_VERSION < version.parse("2023.9.0"):
    KEYWORDS_IN_FILENAME_BASE_PATTERNS = ...
    KEYWORDS_IN_DIR_NAME_BASE_PATTERNS = ...
else:
    KEYWORDS_IN_FILENAME_BASE_PATTERNS = ...
    KEYWORDS_IN_DIR_NAME_BASE_PATTERNS = ...
    KEYWORDS_IN_FILENAME_BASE_PATTERNS = ...
    KEYWORDS_IN_DIR_NAME_BASE_PATTERNS = ...
DEFAULT_SPLITS = ...
DEFAULT_PATTERNS_SPLIT_IN_FILENAME = ...
DEFAULT_PATTERNS_SPLIT_IN_DIR_NAME = ...
DEFAULT_PATTERNS_ALL = ...
ALL_SPLIT_PATTERNS = ...
ALL_DEFAULT_PATTERNS = ...
WILDCARD_CHARACTERS = ...
FILES_TO_IGNORE = ...
def contains_wildcards(pattern: str) -> bool:
    ...

def sanitize_patterns(patterns: Union[dict, list, str]) -> dict[str, Union[list[str], DataFilesList]]:
    """
    Take the data_files patterns from the user, and format them into a dictionary.
    Each key is the name of the split, and each value is a list of data files patterns (paths or urls).
    The default split is "train".

    Returns:
        patterns: dictionary of split_name -> list of patterns
    """
    ...

def resolve_pattern(pattern: str, base_path: str, allowed_extensions: Optional[list[str]] = ..., download_config: Optional[DownloadConfig] = ...) -> list[str]:
    """
    Resolve the paths and URLs of the data files from the pattern passed by the user.

    You can use patterns to resolve multiple local files. Here are a few examples:
    - *.csv to match all the CSV files at the first level
    - **.csv to match all the CSV files at any level
    - data/* to match all the files inside "data"
    - data/** to match all the files inside "data" and its subdirectories

    The patterns are resolved using the fsspec glob. In fsspec>=2023.12.0 this is equivalent to
    Python's glob.glob, Path.glob, Path.match and fnmatch where ** is unsupported with a prefix/suffix
    other than a forward slash /.

    More generally:
    - '*' matches any character except a forward-slash (to match just the file or directory name)
    - '**' matches any character including a forward-slash /

    Hidden files and directories (i.e. whose names start with a dot) are ignored, unless they are explicitly requested.
    The same applies to special directories that start with a double underscore like "__pycache__".
    You can still include one if the pattern explicitly mentions it:
    - to include a hidden file: "*/.hidden.txt" or "*/.*"
    - to include a hidden directory: ".hidden/*" or ".*/*"
    - to include a special directory: "__special__/*" or "__*/*"

    Example::

        >>> from datasets.data_files import resolve_pattern
        >>> base_path = "."
        >>> resolve_pattern("docs/**/*.py", base_path)
        [/Users/mariosasko/Desktop/projects/datasets/docs/source/_config.py']

    Args:
        pattern (str): Unix pattern or paths or URLs of the data files to resolve.
            The paths can be absolute or relative to base_path.
            Remote filesystems using fsspec are supported, e.g. with the hf:// protocol.
        base_path (str): Base path to use when resolving relative paths.
        allowed_extensions (Optional[list], optional): White-list of file extensions to use. Defaults to None (all extensions).
            For example: allowed_extensions=[".csv", ".json", ".txt", ".parquet"]
        download_config ([`DownloadConfig`], *optional*): Specific download configuration parameters.
    Returns:
        List[str]: List of paths or URLs to the local or remote files that match the patterns.
    """
    ...

def get_data_patterns(base_path: str, download_config: Optional[DownloadConfig] = ...) -> dict[str, list[str]]:
    """
    Get the default pattern from a directory testing all the supported patterns.
    The first patterns to return a non-empty list of data files is returned.

    Some examples of supported patterns:

    Input:

        my_dataset_repository/
        ├── README.md
        └── dataset.csv

    Output:

        {'train': ['**']}

    Input:

        my_dataset_repository/
        ├── README.md
        ├── train.csv
        └── test.csv

        my_dataset_repository/
        ├── README.md
        └── data/
            ├── train.csv
            └── test.csv

        my_dataset_repository/
        ├── README.md
        ├── train_0.csv
        ├── train_1.csv
        ├── train_2.csv
        ├── train_3.csv
        ├── test_0.csv
        └── test_1.csv

    Output:

        {'train': ['**/train[-._ 0-9]*', '**/*[-._ 0-9]train[-._ 0-9]*', '**/training[-._ 0-9]*', '**/*[-._ 0-9]training[-._ 0-9]*'],
         'test': ['**/test[-._ 0-9]*', '**/*[-._ 0-9]test[-._ 0-9]*', '**/testing[-._ 0-9]*', '**/*[-._ 0-9]testing[-._ 0-9]*', ...]}

    Input:

        my_dataset_repository/
        ├── README.md
        └── data/
            ├── train/
            │   ├── shard_0.csv
            │   ├── shard_1.csv
            │   ├── shard_2.csv
            │   └── shard_3.csv
            └── test/
                ├── shard_0.csv
                └── shard_1.csv

    Output:

        {'train': ['**/train/**', '**/train[-._ 0-9]*/**', '**/*[-._ 0-9]train/**', '**/*[-._ 0-9]train[-._ 0-9]*/**', ...],
         'test': ['**/test/**', '**/test[-._ 0-9]*/**', '**/*[-._ 0-9]test/**', '**/*[-._ 0-9]test[-._ 0-9]*/**', ...]}

    Input:

        my_dataset_repository/
        ├── README.md
        └── data/
            ├── train-00000-of-00003.csv
            ├── train-00001-of-00003.csv
            ├── train-00002-of-00003.csv
            ├── test-00000-of-00001.csv
            ├── random-00000-of-00003.csv
            ├── random-00001-of-00003.csv
            └── random-00002-of-00003.csv

    Output:

        {'train': ['data/train-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*'],
         'test': ['data/test-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*'],
         'random': ['data/random-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*']}

    In order, it first tests if SPLIT_PATTERN_SHARDED works, otherwise it tests the patterns in ALL_DEFAULT_PATTERNS.
    """
    ...

class DataFilesList(list[str]):
    """
    List of data files (absolute local paths or URLs).
    It has two construction methods given the user's data files patterns:
    - ``from_hf_repo``: resolve patterns inside a dataset repository
    - ``from_local_or_remote``: resolve patterns from a local path

    Moreover, DataFilesList has an additional attribute ``origin_metadata``.
    It can store:
    - the last modified time of local files
    - ETag of remote files
    - commit sha of a dataset repository

    Thanks to this additional attribute, it is possible to hash the list
    and get a different hash if and only if at least one file changed.
    This is useful for caching Dataset objects that are obtained from a list of data files.
    """
    def __init__(self, data_files: list[str], origin_metadata: list[SingleOriginMetadata]) -> None:
        ...
    
    def __add__(self, other: DataFilesList) -> DataFilesList:
        ...
    
    @classmethod
    def from_hf_repo(cls, patterns: list[str], dataset_info: huggingface_hub.hf_api.DatasetInfo, base_path: Optional[str] = ..., allowed_extensions: Optional[list[str]] = ..., download_config: Optional[DownloadConfig] = ...) -> DataFilesList:
        ...
    
    @classmethod
    def from_local_or_remote(cls, patterns: list[str], base_path: Optional[str] = ..., allowed_extensions: Optional[list[str]] = ..., download_config: Optional[DownloadConfig] = ...) -> DataFilesList:
        ...
    
    @classmethod
    def from_patterns(cls, patterns: list[str], base_path: Optional[str] = ..., allowed_extensions: Optional[list[str]] = ..., download_config: Optional[DownloadConfig] = ...) -> DataFilesList:
        ...
    
    def filter(self, *, extensions: Optional[list[str]] = ..., file_names: Optional[list[str]] = ...) -> DataFilesList:
        ...
    


class DataFilesDict(dict[str, DataFilesList]):
    """
    Dict of split_name -> list of data files (absolute local paths or URLs).
    It has two construction methods given the user's data files patterns :
    - ``from_hf_repo``: resolve patterns inside a dataset repository
    - ``from_local_or_remote``: resolve patterns from a local path

    Moreover, each list is a DataFilesList. It is possible to hash the dictionary
    and get a different hash if and only if at least one file changed.
    For more info, see [`DataFilesList`].

    This is useful for caching Dataset objects that are obtained from a list of data files.

    Changing the order of the keys of this dictionary also doesn't change its hash.
    """
    @classmethod
    def from_local_or_remote(cls, patterns: dict[str, Union[list[str], DataFilesList]], base_path: Optional[str] = ..., allowed_extensions: Optional[list[str]] = ..., download_config: Optional[DownloadConfig] = ...) -> DataFilesDict:
        ...
    
    @classmethod
    def from_hf_repo(cls, patterns: dict[str, Union[list[str], DataFilesList]], dataset_info: huggingface_hub.hf_api.DatasetInfo, base_path: Optional[str] = ..., allowed_extensions: Optional[list[str]] = ..., download_config: Optional[DownloadConfig] = ...) -> DataFilesDict:
        ...
    
    @classmethod
    def from_patterns(cls, patterns: dict[str, Union[list[str], DataFilesList]], base_path: Optional[str] = ..., allowed_extensions: Optional[list[str]] = ..., download_config: Optional[DownloadConfig] = ...) -> DataFilesDict:
        ...
    
    def filter(self, *, extensions: Optional[list[str]] = ..., file_names: Optional[list[str]] = ...) -> DataFilesDict:
        ...
    


class DataFilesPatternsList(list[str]):
    """
    List of data files patterns (absolute local paths or URLs).
    For each pattern there should also be a list of allowed extensions
    to keep, or a None ot keep all the files for the pattern.
    """
    def __init__(self, patterns: list[str], allowed_extensions: list[Optional[list[str]]]) -> None:
        ...
    
    def __add__(self, other): # -> DataFilesList:
        ...
    
    @classmethod
    def from_patterns(cls, patterns: list[str], allowed_extensions: Optional[list[str]] = ...) -> DataFilesPatternsList:
        ...
    
    def resolve(self, base_path: str, download_config: Optional[DownloadConfig] = ...) -> DataFilesList:
        ...
    
    def filter_extensions(self, extensions: list[str]) -> DataFilesPatternsList:
        ...
    


class DataFilesPatternsDict(dict[str, DataFilesPatternsList]):
    """
    Dict of split_name -> list of data files patterns (absolute local paths or URLs).
    """
    @classmethod
    def from_patterns(cls, patterns: dict[str, list[str]], allowed_extensions: Optional[list[str]] = ...) -> DataFilesPatternsDict:
        ...
    
    def resolve(self, base_path: str, download_config: Optional[DownloadConfig] = ...) -> DataFilesDict:
        ...
    
    def filter_extensions(self, extensions: list[str]) -> DataFilesPatternsDict:
        ...
    


