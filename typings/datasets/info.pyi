"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from typing import ClassVar, Optional, Union
from huggingface_hub import DatasetCardData
from .features import Features
from .utils import Version

"""DatasetInfo record information we know about a dataset.

This includes things that we know about the dataset statically, i.e.:
 - description
 - canonical location
 - does it have validation and tests splits
 - size
 - etc.

This also includes the things that can and should be computed once we've
processed the dataset as well:
 - number of examples (in each split)
 - etc.
"""
logger = ...
@dataclass
class SupervisedKeysData:
    input: str = ...
    output: str = ...


@dataclass
class DownloadChecksumsEntryData:
    key: str = ...
    value: str = ...


class MissingCachedSizesConfigError(Exception):
    """The expected cached sizes of the download file are missing."""
    ...


class NonMatchingCachedSizesError(Exception):
    """The prepared split doesn't have expected sizes."""
    ...


@dataclass
class PostProcessedInfo:
    features: Optional[Features] = ...
    resources_checksums: Optional[dict] = ...
    def __post_init__(self): # -> None:
        ...
    
    @classmethod
    def from_dict(cls, post_processed_info_dict: dict) -> PostProcessedInfo:
        ...
    


@dataclass
class DatasetInfo:
    """Information about a dataset.

    `DatasetInfo` documents datasets, including its name, version, and features.
    See the constructor arguments and properties for a full list.

    Not all fields are known on construction and may be updated later.

    Attributes:
        description (`str`):
            A description of the dataset.
        citation (`str`):
            A BibTeX citation of the dataset.
        homepage (`str`):
            A URL to the official homepage for the dataset.
        license (`str`):
            The dataset's license. It can be the name of the license or a paragraph containing the terms of the license.
        features ([`Features`], *optional*):
            The features used to specify the dataset's column types.
        post_processed (`PostProcessedInfo`, *optional*):
            Information regarding the resources of a possible post-processing of a dataset. For example, it can contain the information of an index.
        supervised_keys (`SupervisedKeysData`, *optional*):
            Specifies the input feature and the label for supervised learning if applicable for the dataset (legacy from TFDS).
        builder_name (`str`, *optional*):
            The name of the `GeneratorBasedBuilder` subclass used to create the dataset. It is also the snake_case version of the dataset builder class name.
        config_name (`str`, *optional*):
            The name of the configuration derived from [`BuilderConfig`].
        version (`str` or [`Version`], *optional*):
            The version of the dataset.
        splits (`dict`, *optional*):
            The mapping between split name and metadata.
        download_checksums (`dict`, *optional*):
            The mapping between the URL to download the dataset's checksums and corresponding metadata.
        download_size (`int`, *optional*):
            The size of the files to download to generate the dataset, in bytes.
        post_processing_size (`int`, *optional*):
            Size of the dataset in bytes after post-processing, if any.
        dataset_size (`int`, *optional*):
            The combined size in bytes of the Arrow tables for all splits.
        size_in_bytes (`int`, *optional*):
            The combined size in bytes of all files associated with the dataset (downloaded files + Arrow files).
        **config_kwargs (additional keyword arguments):
            Keyword arguments to be passed to the [`BuilderConfig`] and used in the [`DatasetBuilder`].
    """
    description: str = ...
    citation: str = ...
    homepage: str = ...
    license: str = ...
    features: Optional[Features] = ...
    post_processed: Optional[PostProcessedInfo] = ...
    supervised_keys: Optional[SupervisedKeysData] = ...
    builder_name: Optional[str] = ...
    dataset_name: Optional[str] = ...
    config_name: Optional[str] = ...
    version: Optional[Union[str, Version]] = ...
    splits: Optional[dict] = ...
    download_checksums: Optional[dict] = ...
    download_size: Optional[int] = ...
    post_processing_size: Optional[int] = ...
    dataset_size: Optional[int] = ...
    size_in_bytes: Optional[int] = ...
    _INCLUDED_INFO_IN_YAML: ClassVar[list[str]] = ...
    def __post_init__(self): # -> None:
        ...
    
    def write_to_directory(self, dataset_info_dir, pretty_print=..., storage_options: Optional[dict] = ...): # -> None:
        """Write `DatasetInfo` and license (if present) as JSON files to `dataset_info_dir`.

        Args:
            dataset_info_dir (`str`):
                Destination directory.
            pretty_print (`bool`, defaults to `False`):
                If `True`, the JSON will be pretty-printed with the indent level of 4.
            storage_options (`dict`, *optional*):
                Key/value pairs to be passed on to the file-system backend, if any.

                <Added version="2.9.0"/>

        Example:

        ```py
        >>> from datasets import load_dataset
        >>> ds = load_dataset("cornell-movie-review-data/rotten_tomatoes", split="validation")
        >>> ds.info.write_to_directory("/path/to/directory/")
        ```
        """
        ...
    
    @classmethod
    def from_merge(cls, dataset_infos: list[DatasetInfo]): # -> DatasetInfo | Self:
        ...
    
    @classmethod
    def from_directory(cls, dataset_info_dir: str, storage_options: Optional[dict] = ...) -> DatasetInfo:
        """Create [`DatasetInfo`] from the JSON file in `dataset_info_dir`.

        This function updates all the dynamically generated fields (num_examples,
        hash, time of creation,...) of the [`DatasetInfo`].

        This will overwrite all previous metadata.

        Args:
            dataset_info_dir (`str`):
                The directory containing the metadata file. This
                should be the root directory of a specific dataset version.
            storage_options (`dict`, *optional*):
                Key/value pairs to be passed on to the file-system backend, if any.

                <Added version="2.9.0"/>

        Example:

        ```py
        >>> from datasets import DatasetInfo
        >>> ds_info = DatasetInfo.from_directory("/path/to/directory/")
        ```
        """
        ...
    
    @classmethod
    def from_dict(cls, dataset_info_dict: dict) -> DatasetInfo:
        ...
    
    def update(self, other_dataset_info: DatasetInfo, ignore_none=...): # -> None:
        ...
    
    def copy(self) -> DatasetInfo:
        ...
    


class DatasetInfosDict(dict[str, DatasetInfo]):
    def write_to_directory(self, dataset_infos_dir, overwrite=..., pretty_print=...) -> None:
        ...
    
    @classmethod
    def from_directory(cls, dataset_infos_dir) -> DatasetInfosDict:
        ...
    
    @classmethod
    def from_dataset_card_data(cls, dataset_card_data: DatasetCardData) -> DatasetInfosDict:
        ...
    
    def to_dataset_card_data(self, dataset_card_data: DatasetCardData) -> None:
        ...
    


