"""
This type stub file was generated by pyright.
"""

import numpy as np
import pyarrow as pa
from collections.abc import Iterable
from dataclasses import InitVar, dataclass
from typing import Any, ClassVar, Literal, Optional, Union
from pandas.api.extensions import ExtensionArray as PandasExtensionArray, ExtensionDtype as PandasExtensionDtype
from ..utils import experimental
from .audio import Audio
from .image import Image
from .pdf import Pdf
from .translation import Translation, TranslationVariableLanguages
from .video import Video

"""This class handle features definition in datasets and some utilities to display table type."""
logger = ...
def string_to_arrow(datasets_dtype: str) -> pa.DataType:
    """
    string_to_arrow takes a datasets string dtype and converts it to a pyarrow.DataType.

    In effect, `dt == string_to_arrow(_arrow_to_datasets_dtype(dt))`

    This is necessary because the datasets.Value() primitive type is constructed using a string dtype

    Value(dtype=str)

    But Features.type (via `get_nested_type()` expects to resolve Features into a pyarrow Schema,
        which means that each Value() must be able to resolve into a corresponding pyarrow.DataType, which is the
        purpose of this function.
    """
    ...

def cast_to_python_objects(obj: Any, only_1d_for_numpy=..., optimize_list_casting=...) -> Any:
    """
    Cast numpy/pytorch/tensorflow/pandas objects to python lists.
    It works recursively.

    If `optimize_list_casting` is True, To avoid iterating over possibly long lists, it first checks (recursively) if the first element that is not None or empty (if it is a sequence) has to be casted.
    If the first element needs to be casted, then all the elements of the list will be casted, otherwise they'll stay the same.
    This trick allows to cast objects that contain tokenizers outputs without iterating over every single token for example.

    Args:
        obj: the object (nested struct) to cast
        only_1d_for_numpy (bool, default ``False``): whether to keep the full multi-dim tensors as multi-dim numpy arrays, or convert them to
            nested lists of 1-dimensional numpy arrays. This can be useful to keep only 1-d arrays to instantiate Arrow arrays.
            Indeed Arrow only support converting 1-dimensional array values.
        optimize_list_casting (bool, default ``True``): whether to optimize list casting by checking the first non-null element to see if it needs to be casted
            and if it doesn't, not checking the rest of the list elements.

    Returns:
        casted_obj: the casted object
    """
    ...

@dataclass(repr=False)
class Value:
    """
    Scalar feature value of a particular data type.

    The possible dtypes of `Value` are as follows:
    - `null`
    - `bool`
    - `int8`
    - `int16`
    - `int32`
    - `int64`
    - `uint8`
    - `uint16`
    - `uint32`
    - `uint64`
    - `float16`
    - `float32` (alias float)
    - `float64` (alias double)
    - `time32[(s|ms)]`
    - `time64[(us|ns)]`
    - `timestamp[(s|ms|us|ns)]`
    - `timestamp[(s|ms|us|ns), tz=(tzstring)]`
    - `date32`
    - `date64`
    - `duration[(s|ms|us|ns)]`
    - `decimal128(precision, scale)`
    - `decimal256(precision, scale)`
    - `binary`
    - `large_binary`
    - `binary_view`
    - `string`
    - `large_string`
    - `string_view`

    Args:
        dtype (`str`):
            Name of the data type.

    Example:

    ```py
    >>> from datasets import Features
    >>> features = Features({'stars': Value('int32')})
    >>> features
    {'stars': Value('int32')}
    ```
    """
    dtype: str
    id: Optional[str] = ...
    pa_type: ClassVar[Any] = ...
    _type: str = ...
    def __post_init__(self): # -> None:
        ...
    
    def __call__(self): # -> Any:
        ...
    
    def encode_example(self, value): # -> bool | int | float | str:
        ...
    
    def __repr__(self): # -> str:
        ...
    


class _ArrayXD:
    def __post_init__(self): # -> None:
        ...
    
    def __call__(self): # -> Any:
        ...
    
    def encode_example(self, value):
        ...
    


@dataclass
class Array2D(_ArrayXD):
    """Create a two-dimensional array.

    Args:
        shape (`tuple`):
            Size of each dimension.
        dtype (`str`):
            Name of the data type.

    Example:

    ```py
    >>> from datasets import Features
    >>> features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})
    ```
    """
    shape: tuple
    dtype: str
    id: Optional[str] = ...
    _type: str = ...


@dataclass
class Array3D(_ArrayXD):
    """Create a three-dimensional array.

    Args:
        shape (`tuple`):
            Size of each dimension.
        dtype (`str`):
            Name of the data type.

    Example:

    ```py
    >>> from datasets import Features
    >>> features = Features({'x': Array3D(shape=(1, 2, 3), dtype='int32')})
    ```
    """
    shape: tuple
    dtype: str
    id: Optional[str] = ...
    _type: str = ...


@dataclass
class Array4D(_ArrayXD):
    """Create a four-dimensional array.

    Args:
        shape (`tuple`):
            Size of each dimension.
        dtype (`str`):
            Name of the data type.

    Example:

    ```py
    >>> from datasets import Features
    >>> features = Features({'x': Array4D(shape=(1, 2, 2, 3), dtype='int32')})
    ```
    """
    shape: tuple
    dtype: str
    id: Optional[str] = ...
    _type: str = ...


@dataclass
class Array5D(_ArrayXD):
    """Create a five-dimensional array.

    Args:
        shape (`tuple`):
            Size of each dimension.
        dtype (`str`):
            Name of the data type.

    Example:

    ```py
    >>> from datasets import Features
    >>> features = Features({'x': Array5D(shape=(1, 2, 2, 3, 3), dtype='int32')})
    ```
    """
    shape: tuple
    dtype: str
    id: Optional[str] = ...
    _type: str = ...


class _ArrayXDExtensionType(pa.ExtensionType):
    ndims: Optional[int] = ...
    def __init__(self, shape: tuple, dtype: str) -> None:
        ...
    
    def __arrow_ext_serialize__(self): # -> bytes:
        ...
    
    @classmethod
    def __arrow_ext_deserialize__(cls, storage_type, serialized): # -> Self:
        ...
    
    def __reduce__(self): # -> tuple[Callable[..., Self], tuple[Any, bytes]]:
        ...
    
    def __hash__(self) -> int:
        ...
    
    def __arrow_ext_class__(self): # -> type[ArrayExtensionArray]:
        ...
    
    def to_pandas_dtype(self): # -> PandasArrayExtensionDtype:
        ...
    


class Array2DExtensionType(_ArrayXDExtensionType):
    ndims = ...


class Array3DExtensionType(_ArrayXDExtensionType):
    ndims = ...


class Array4DExtensionType(_ArrayXDExtensionType):
    ndims = ...


class Array5DExtensionType(_ArrayXDExtensionType):
    ndims = ...


class ArrayExtensionArray(pa.ExtensionArray):
    def __array__(self): # -> NDArray[Any] | _Array1D[Any]:
        ...
    
    def __getitem__(self, i):
        ...
    
    def to_numpy(self, zero_copy_only=...): # -> NDArray[Any] | _Array1D[Any]:
        ...
    
    def to_pylist(self, maps_as_pydicts: Optional[Literal["lossy", "strict"]] = ...): # -> list[Any] | Any:
        ...
    


class PandasArrayExtensionDtype(PandasExtensionDtype):
    _metadata = ...
    def __init__(self, value_type: Union[PandasArrayExtensionDtype, np.dtype]) -> None:
        ...
    
    def __from_arrow__(self, array: Union[pa.Array, pa.ChunkedArray]): # -> PandasArrayExtensionArray:
        ...
    
    @classmethod
    def construct_array_type(cls): # -> type[PandasArrayExtensionArray]:
        ...
    
    @property
    def type(self) -> type:
        ...
    
    @property
    def kind(self) -> str:
        ...
    
    @property
    def name(self) -> str:
        ...
    
    @property
    def value_type(self) -> np.dtype:
        ...
    


class PandasArrayExtensionArray(PandasExtensionArray):
    def __init__(self, data: np.ndarray, copy: bool = ...) -> None:
        ...
    
    def __array__(self, dtype=...): # -> _Array1D[Any] | ndarray[_AnyShape, dtype[Any]]:
        """
        Convert to NumPy Array.
        Note that Pandas expects a 1D array when dtype is set to object.
        But for other dtypes, the returned shape is the same as the one of ``data``.

        More info about pandas 1D requirement for PandasExtensionArray here:
        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.extensions.ExtensionArray.html#pandas.api.extensions.ExtensionArray

        """
        ...
    
    def copy(self, deep: bool = ...) -> PandasArrayExtensionArray:
        ...
    
    @property
    def dtype(self) -> PandasArrayExtensionDtype:
        ...
    
    @property
    def nbytes(self) -> int:
        ...
    
    def isna(self) -> np.ndarray:
        ...
    
    def __setitem__(self, key: Union[int, slice, np.ndarray], value: Any) -> None:
        ...
    
    def __getitem__(self, item: Union[int, slice, np.ndarray]) -> Union[np.ndarray, PandasArrayExtensionArray]:
        ...
    
    def take(self, indices: Sequence_[int], allow_fill: bool = ..., fill_value: bool = ...) -> PandasArrayExtensionArray:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __eq__(self, other) -> np.ndarray:
        ...
    


def pandas_types_mapper(dtype): # -> PandasArrayExtensionDtype | None:
    ...

@dataclass
class ClassLabel:
    """Feature type for integer class labels.

    There are 3 ways to define a `ClassLabel`, which correspond to the 3 arguments:

     * `num_classes`: Create 0 to (num_classes-1) labels.
     * `names`: List of label strings.
     * `names_file`: File containing the list of labels.

    Under the hood the labels are stored as integers.
    You can use negative integers to represent unknown/missing labels.

    Args:
        num_classes (`int`, *optional*):
            Number of classes. All labels must be < `num_classes`.
        names (`list` of `str`, *optional*):
            String names for the integer classes.
            The order in which the names are provided is kept.
        names_file (`str`, *optional*):
            Path to a file with names for the integer classes, one per line.

    Example:

    ```py
    >>> from datasets import Features, ClassLabel
    >>> features = Features({'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'])})
    >>> features
    {'label': ClassLabel(names=['bad', 'ok', 'good'])}
    ```
    """
    num_classes: InitVar[Optional[int]] = ...
    names: list[str] = ...
    names_file: InitVar[Optional[str]] = ...
    id: Optional[str] = ...
    dtype: ClassVar[str] = ...
    pa_type: ClassVar[Any] = ...
    _str2int: ClassVar[dict[str, int]] = ...
    _int2str: ClassVar[dict[int, int]] = ...
    _type: str = ...
    def __post_init__(self, num_classes, names_file): # -> None:
        ...
    
    def __call__(self): # -> Any:
        ...
    
    def str2int(self, values: Union[str, Iterable]) -> Union[int, Iterable]:
        """Conversion class name `string` => `integer`.

        Example:

        ```py
        >>> from datasets import load_dataset
        >>> ds = load_dataset("cornell-movie-review-data/rotten_tomatoes", split="train")
        >>> ds.features["label"].str2int('neg')
        0
        ```
        """
        ...
    
    def int2str(self, values: Union[int, Iterable]) -> Union[str, Iterable]:
        """Conversion `integer` => class name `string`.

        Regarding unknown/missing labels: passing negative integers raises `ValueError`.

        Example:

        ```py
        >>> from datasets import load_dataset
        >>> ds = load_dataset("cornell-movie-review-data/rotten_tomatoes", split="train")
        >>> ds.features["label"].int2str(0)
        'neg'
        ```
        """
        ...
    
    def encode_example(self, example_data): # -> int | Iterable[Any]:
        ...
    
    def cast_storage(self, storage: Union[pa.StringArray, pa.IntegerArray]) -> pa.Int64Array:
        """Cast an Arrow array to the `ClassLabel` arrow storage type.
        The Arrow types that can be converted to the `ClassLabel` pyarrow storage type are:

        - `pa.string()`
        - `pa.int()`

        Args:
            storage (`Union[pa.StringArray, pa.IntegerArray]`):
                PyArrow array to cast.

        Returns:
            `pa.Int64Array`: Array in the `ClassLabel` arrow storage type.
        """
        ...
    


class Sequence:
    """
    A `Sequence` is a utility that automatically converts internal dictionary feature into a dictionary of
    lists. This behavior is implemented to have a compatibility layer with the TensorFlow Datasets library but may be
    un-wanted in some cases. If you don't want this behavior, you can use a [`List`] or a [`LargeList`]
    instead of the [`Sequence`].

    Args:
        feature ([`FeatureType`]):
            Child feature data type of each item within the large list.
        length (optional `int`, default to -1):
            Length of the list if it is fixed.
            Defaults to -1 which means an arbitrary length.

        Returns:
            [`List`] of the specified feature, except `dict` of sub-features
            which are converted to `dict` of lists of sub-features for compatibility with TFDS.

    """
    def __new__(cls, feature=..., length=..., **kwargs): # -> dict[Any, List] | Self:
        ...
    


@dataclass(repr=False)
class List(Sequence):
    """Feature type for large list data composed of child feature data type.

    It is backed by `pyarrow.ListType`, which uses 32-bit offsets or a fixed length.

    Args:
        feature ([`FeatureType`]):
            Child feature data type of each item within the large list.
        length (optional `int`, default to -1):
            Length of the list if it is fixed.
            Defaults to -1 which means an arbitrary length.
    """
    feature: Any
    length: int = ...
    id: Optional[str] = ...
    pa_type: ClassVar[Any] = ...
    _type: str = ...
    def __repr__(self): # -> str:
        ...
    


@dataclass(repr=False)
class LargeList:
    """Feature type for large list data composed of child feature data type.

    It is backed by `pyarrow.LargeListType`, which is like `pyarrow.ListType` but with 64-bit rather than 32-bit offsets.

    Args:
        feature ([`FeatureType`]):
            Child feature data type of each item within the large list.
    """
    feature: Any
    id: Optional[str] = ...
    pa_type: ClassVar[Any] = ...
    _type: str = ...
    def __repr__(self): # -> str:
        ...
    


FeatureType = Union[dict, list, tuple, Value, ClassLabel, Translation, TranslationVariableLanguages, LargeList, List, Array2D, Array3D, Array4D, Array5D, Audio, Image, Video, Pdf,]
def get_nested_type(schema: FeatureType) -> pa.DataType:
    """
    get_nested_type() converts a datasets.FeatureType into a pyarrow.DataType, and acts as the inverse of
        generate_from_arrow_type().

    It performs double-duty as the implementation of Features.type and handles the conversion of
        datasets.Feature->pa.struct
    """
    ...

def encode_nested_example(schema, obj, level=...): # -> dict[Any, dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[dict[Any, dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | None:
    """Encode a nested example.
    This is used since some features (in particular ClassLabel) have some logic during encoding.

    To avoid iterating over possibly long lists, it first checks (recursively) if the first element that is not None or empty (if it is a sequence) has to be encoded.
    If the first element needs to be encoded, then all the elements of the list will be encoded, otherwise they'll stay the same.
    """
    ...

def decode_nested_example(schema, obj, token_per_repo_id: Optional[dict[str, Union[str, bool, None]]] = ...): # -> dict[Any, dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[dict[Any, dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | None:
    """Decode a nested example.
    This is used since some features (in particular Audio and Image) have some logic during decoding.

    To avoid iterating over possibly long lists, it first checks (recursively) if the first element that is not None or empty (if it is a sequence) has to be decoded.
    If the first element needs to be decoded, then all the elements of the list will be decoded, otherwise they'll stay the same.
    """
    ...

_FEATURE_TYPES: dict[str, FeatureType] = ...
@experimental
def register_feature(feature_cls: type, feature_type: str): # -> None:
    """
    Register a Feature object using a name and class.
    This function must be used on a Feature class.
    """
    ...

def generate_from_dict(obj: Any): # -> list[list[Any] | dict[Any, list[Any] | dict[Any, Any] | LargeList | List | dict[Any, List] | Sequence | Any] | LargeList | List | dict[Any, List] | Sequence | Any] | dict[Any, list[list[Any] | dict[Any, Any] | LargeList | List | dict[Any, List] | Sequence | Any] | dict[Any, Any] | LargeList | List | dict[Any, List] | Sequence | Any] | LargeList | List | dict[Any, List] | Sequence | Any:
    """Regenerate the nested feature object from a deserialized dict.
    We use the '_type' fields to get the dataclass name to load.

    generate_from_dict is the recursive helper for Features.from_dict, and allows for a convenient constructor syntax
    to define features from deserialized JSON dictionaries. This function is used in particular when deserializing
    a :class:`DatasetInfo` that was dumped to a JSON object. This acts as an analogue to
    :meth:`Features.from_arrow_schema` and handles the recursive field-by-field instantiation, but doesn't require any
    mapping to/from pyarrow, except for the fact that it takes advantage of the mapping of pyarrow primitive dtypes
    that :class:`Value` automatically performs.
    """
    ...

def generate_from_arrow_type(pa_type: pa.DataType) -> FeatureType:
    """
    generate_from_arrow_type accepts an arrow DataType and returns a datasets FeatureType to be used as the type for
        a single field.

    This is the high-level arrow->datasets type conversion and is inverted by get_nested_type().

    This operates at the individual *field* level, whereas Features.from_arrow_schema() operates at the
        full schema level and holds the methods that represent the bijection from Features<->pyarrow.Schema
    """
    ...

def numpy_to_pyarrow_listarray(arr: np.ndarray, type: pa.DataType = ...) -> pa.ListArray:
    """Build a PyArrow ListArray from a multidimensional NumPy array"""
    ...

def list_of_pa_arrays_to_pyarrow_listarray(l_arr: list[Optional[pa.Array]]) -> pa.ListArray:
    ...

def list_of_np_array_to_pyarrow_listarray(l_arr: list[np.ndarray], type: pa.DataType = ...) -> pa.ListArray:
    """Build a PyArrow ListArray from a possibly nested list of NumPy arrays"""
    ...

def contains_any_np_array(data: Any): # -> bool:
    """Return `True` if data is a NumPy ndarray or (recursively) if first non-null value in list is a NumPy ndarray.

    Args:
        data (Any): Data.

    Returns:
        bool
    """
    ...

def any_np_array_to_pyarrow_listarray(data: Union[np.ndarray, list], type: pa.DataType = ...) -> pa.ListArray:
    """Convert to PyArrow ListArray either a NumPy ndarray or (recursively) a list that may contain any NumPy ndarray.

    Args:
        data (Union[np.ndarray, List]): Data.
        type (pa.DataType): Explicit PyArrow DataType passed to coerce the ListArray data type.

    Returns:
        pa.ListArray
    """
    ...

def to_pyarrow_listarray(data: Any, pa_type: _ArrayXDExtensionType) -> pa.Array:
    """Convert to PyArrow ListArray.

    Args:
        data (Any): List, iterable, np.ndarray or pd.Series.
        pa_type (_ArrayXDExtensionType): Any of the ArrayNDExtensionType.

    Returns:
        pyarrow.Array
    """
    ...

_VisitPath = list[Union[str, Literal[0]]]
def require_decoding(feature: FeatureType, ignore_decode_attribute: bool = ...) -> bool:
    """Check if a (possibly nested) feature requires decoding.

    Args:
        feature (FeatureType): the feature type to be checked
        ignore_decode_attribute (:obj:`bool`, default ``False``): Whether to ignore the current value
            of the `decode` attribute of the decodable feature types.
    Returns:
        :obj:`bool`
    """
    ...

def require_storage_cast(feature: FeatureType) -> bool:
    """Check if a (possibly nested) feature requires storage casting.

    Args:
        feature (FeatureType): the feature type to be checked
    Returns:
        :obj:`bool`
    """
    ...

def require_storage_embed(feature: FeatureType) -> bool:
    """Check if a (possibly nested) feature requires embedding data into storage.

    Args:
        feature (FeatureType): the feature type to be checked
    Returns:
        :obj:`bool`
    """
    ...

def keep_features_dicts_synced(func): # -> _Wrapped[..., Any, ..., Any]:
    """
    Wrapper to keep the secondary dictionary, which tracks whether keys are decodable, of the :class:`datasets.Features` object
    in sync with the main dictionary.
    """
    ...

class Features(dict):
    """A special dictionary that defines the internal structure of a dataset.

    Instantiated with a dictionary of type `dict[str, FieldType]`, where keys are the desired column names,
    and values are the type of that column.

    `FieldType` can be one of the following:
        - [`Value`] feature specifies a single data type value, e.g. `int64` or `string`.
        - [`ClassLabel`] feature specifies a predefined set of classes which can have labels associated to them and
          will be stored as integers in the dataset.
        - Python `dict` specifies a composite feature containing a mapping of sub-fields to sub-features.
          It's possible to have nested fields of nested fields in an arbitrary manner.
        - [`List`] or [`LargeList`] specifies a composite feature containing a sequence of
          sub-features, all of the same feature type.
        - [`Array2D`], [`Array3D`], [`Array4D`] or [`Array5D`] feature for multidimensional arrays.
        - [`Audio`] feature to store the absolute path to an audio file or a dictionary with the relative path
          to an audio file ("path" key) and its bytes content ("bytes" key).
          This feature loads the audio lazily with a decoder.
        - [`Image`] feature to store the absolute path to an image file, an `np.ndarray` object, a `PIL.Image.Image` object
          or a dictionary with the relative path to an image file ("path" key) and its bytes content ("bytes" key).
          This feature extracts the image data.
        - [`Video`] feature to store the absolute path to a video file, a `torchcodec.decoders.VideoDecoder` object
          or a dictionary with the relative path to a video file ("path" key) and its bytes content ("bytes" key).
          This feature loads the video lazily with a decoder.
        - [`Pdf`] feature to store the absolute path to a PDF file, a `pdfplumber.pdf.PDF` object
          or a dictionary with the relative path to a PDF file ("path" key) and its bytes content ("bytes" key).
          This feature loads the PDF lazily with a PDF reader.
        - [`Translation`] or [`TranslationVariableLanguages`] feature specific to Machine Translation.
    """
    def __init__(*args, **kwargs) -> None:
        ...
    
    __setitem__ = ...
    __delitem__ = ...
    update = ...
    setdefault = ...
    pop = ...
    popitem = ...
    clear = ...
    def __reduce__(self): # -> tuple[type[Features], tuple[dict[Any, Any]]]:
        ...
    
    @property
    def type(self):
        """
        Features field types.

        Returns:
            :obj:`pyarrow.DataType`
        """
        ...
    
    @property
    def arrow_schema(self):
        """
        Features schema.

        Returns:
            :obj:`pyarrow.Schema`
        """
        ...
    
    @classmethod
    def from_arrow_schema(cls, pa_schema: pa.Schema) -> Features:
        """
        Construct [`Features`] from Arrow Schema.
        It also checks the schema metadata for Hugging Face Datasets features.
        Non-nullable fields are not supported and set to nullable.

        Also, pa.dictionary is not supported and it uses its underlying type instead.
        Therefore datasets convert DictionaryArray objects to their actual values.

        Args:
            pa_schema (`pyarrow.Schema`):
                Arrow Schema.

        Returns:
            [`Features`]
        """
        ...
    
    @classmethod
    def from_dict(cls, dic) -> Features:
        """
        Construct [`Features`] from dict.

        Regenerate the nested feature object from a deserialized dict.
        We use the `_type` key to infer the dataclass name of the feature `FieldType`.

        It allows for a convenient constructor syntax
        to define features from deserialized JSON dictionaries. This function is used in particular when deserializing
        a [`DatasetInfo`] that was dumped to a JSON object. This acts as an analogue to
        [`Features.from_arrow_schema`] and handles the recursive field-by-field instantiation, but doesn't require
        any mapping to/from pyarrow, except for the fact that it takes advantage of the mapping of pyarrow primitive
        dtypes that [`Value`] automatically performs.

        Args:
            dic (`dict[str, Any]`):
                Python dictionary.

        Returns:
            `Features`

        Example::
            >>> Features.from_dict({'_type': {'dtype': 'string', 'id': None, '_type': 'Value'}})
            {'_type': Value('string')}
        """
        ...
    
    def to_dict(self): # -> dict[Any, Any] | tuple[Any, ...] | list[Any]:
        ...
    
    def encode_example(self, example): # -> dict[Any, dict[Any, Any] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | Any | None] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | None:
        """
        Encode example into a format for Arrow.

        Args:
            example (`dict[str, Any]`):
                Data in a Dataset row.

        Returns:
            `dict[str, Any]`
        """
        ...
    
    def encode_column(self, column, column_name: str): # -> list[dict[Any, dict[Any, Any] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | Any | None] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | Any | None]:
        """
        Encode column into a format for Arrow.

        Args:
            column (`list[Any]`):
                Data in a Dataset column.
            column_name (`str`):
                Dataset column name.

        Returns:
            `list[Any]`
        """
        ...
    
    def encode_batch(self, batch): # -> dict[Any, Any]:
        """
        Encode batch into a format for Arrow.

        Args:
            batch (`dict[str, list[Any]]`):
                Data in a Dataset batch.

        Returns:
            `dict[str, list[Any]]`
        """
        ...
    
    def decode_example(self, example: dict, token_per_repo_id: Optional[dict[str, Union[str, bool, None]]] = ...): # -> dict[Any, dict[Any, dict[Any, Any] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | Any | None] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | Any | None]:
        """Decode example with custom feature decoding.

        Args:
            example (`dict[str, Any]`):
                Dataset row data.
            token_per_repo_id (`dict`, *optional*):
                To access and decode audio or image files from private repositories on the Hub, you can pass
                a dictionary `repo_id (str) -> token (bool or str)`.

        Returns:
            `dict[str, Any]`
        """
        ...
    
    def decode_column(self, column: list, column_name: str, token_per_repo_id: Optional[dict[str, Union[str, bool, None]]] = ...): # -> list[dict[Any, dict[Any, Any] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | Any | None] | list[dict[Any, Any] | list[dict[Any, Any] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any] | Any | None] | list[Any]:
        """Decode column with custom feature decoding.

        Args:
            column (`list[Any]`):
                Dataset column data.
            column_name (`str`):
                Dataset column name.

        Returns:
            `list[Any]`
        """
        ...
    
    def decode_batch(self, batch: dict, token_per_repo_id: Optional[dict[str, Union[str, bool, None]]] = ...): # -> dict[Any, Any]:
        """Decode batch with custom feature decoding.

        Args:
            batch (`dict[str, list[Any]]`):
                Dataset batch data.
            token_per_repo_id (`dict`, *optional*):
                To access and decode audio or image files from private repositories on the Hub, you can pass
                a dictionary repo_id (str) -> token (bool or str)

        Returns:
            `dict[str, list[Any]]`
        """
        ...
    
    def copy(self) -> Features:
        """
        Make a deep copy of [`Features`].

        Returns:
            [`Features`]

        Example:

        ```py
        >>> from datasets import load_dataset
        >>> ds = load_dataset("cornell-movie-review-data/rotten_tomatoes", split="train")
        >>> copy_of_features = ds.features.copy()
        >>> copy_of_features
        {'label': ClassLabel(names=['neg', 'pos']),
         'text': Value('string')}
        ```
        """
        ...
    
    def reorder_fields_as(self, other: Features) -> Features:
        """
        Reorder Features fields to match the field order of other [`Features`].

        The order of the fields is important since it matters for the underlying arrow data.
        Re-ordering the fields allows to make the underlying arrow data type match.

        Args:
            other ([`Features`]):
                The other [`Features`] to align with.

        Returns:
            [`Features`]

        Example::

            >>> from datasets import Features, List, Value
            >>> # let's say we have two features with a different order of nested fields (for a and b for example)
            >>> f1 = Features({"root": {"a": Value("string"), "b": Value("string")}})
            >>> f2 = Features({"root": {"b": Value("string"), "a": Value("string")}})
            >>> assert f1.type != f2.type
            >>> # re-ordering keeps the base structure (here List is defined at the root level), but makes the fields order match
            >>> f1.reorder_fields_as(f2)
            {'root': List({'b': Value('string'), 'a': Value('string')})}
            >>> assert f1.reorder_fields_as(f2).type == f2.type
        """
        ...
    
    def flatten(self, max_depth=...) -> Features:
        """Flatten the features. Every dictionary column is removed and is replaced by
        all the subfields it contains. The new fields are named by concatenating the
        name of the original column and the subfield name like this: `<original>.<subfield>`.

        If a column contains nested dictionaries, then all the lower-level subfields names are
        also concatenated to form new columns: `<original>.<subfield>.<subsubfield>`, etc.

        Returns:
            [`Features`]:
                The flattened features.

        Example:

        ```py
        >>> from datasets import load_dataset
        >>> ds = load_dataset("rajpurkar/squad", split="train")
        >>> ds.features.flatten()
        {'answers.answer_start': List(Value('int32'), id=None),
         'answers.text': List(Value('string'), id=None),
         'context': Value('string'),
         'id': Value('string'),
         'question': Value('string'),
         'title': Value('string')}
        ```
        """
        ...
    


