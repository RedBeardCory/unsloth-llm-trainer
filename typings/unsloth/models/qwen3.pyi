"""
This type stub file was generated by pyright.
"""

from .llama import *

def Qwen3Attention_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: bool = ..., use_cache: bool = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
    ...

torch_matmul = ...
def Qwen3Attention_fast_forward_inference(self, hidden_states: torch.Tensor, past_key_value: Optional[Tuple[torch.Tensor]], position_ids, do_prefill=..., attention_mask=...): # -> tuple[Tensor | Any | None, tuple[Tensor | Any, Tensor | Any]]:
    """
    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
    Fast inference using KV cache.
    QK^T can be computed in 4 chunks

    [Q, q] @ [K, k].T where q, k are the new tokens.
    [QK^T, Qk^T]
    [qK^T, qk^T]

    Since the attention mask wipes Qk^T, we just get
    [QK^T,    0]
    [qK^T, qk^T]

    Since softmax is row-wise, we get
    softmax([QK^T,    0])
    softmax([qK^T, qk^T])

    We then multiply by   [V]
                          [v]
    softmax([QK^T,    0]) [softmax(QK^T)V] *
    softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]

    But notice * [softmax(QK^T)V] is just the last attention.
    We just need to compute the last final row.

    This means we can pass in a row of Q, but we need to
    remember K and V, which are called the KV cache.
    """
    ...

class FastQwen3Model(FastLlamaModel):
    @staticmethod
    def pre_patch(): # -> None:
        ...
    
    @staticmethod
    def from_pretrained(model_name=..., max_seq_length=..., dtype=..., load_in_4bit=..., token=..., device_map=..., rope_scaling=..., fix_tokenizer=..., model_patcher=..., tokenizer_name=..., trust_remote_code=..., **kwargs):
        ...
    


