"""
This type stub file was generated by pyright.
"""

from .llama import *

def fast_layernorm_inference(self, X, out_weight=...):
    ...

def CohereAttention_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: bool = ..., use_cache: bool = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
    ...

def CohereDecoderLayer_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: Optional[bool] = ..., use_cache: Optional[bool] = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs): # -> tuple[Any | Tensor, ...] | tuple[Any | Tensor, Any] | tuple[Any | Tensor]:
    ...

KV_CACHE_INCREMENT = ...
torch_nn_functional_softmax = ...
torch_matmul = ...
def CohereAttention_fast_forward_inference(self, hidden_states: torch.Tensor, past_key_value: Optional[Tuple[torch.Tensor]], position_ids, do_prefill=..., attention_mask=...): # -> tuple[Tensor | Any | None, tuple[Tensor | Any, Tensor | Any]]:
    ...

def CohereModel_fast_forward_inference(self, input_ids, past_key_values, position_ids, attention_mask=...): # -> BaseModelOutputWithPast:
    ...

class FastCohereModel(FastLlamaModel):
    @staticmethod
    def pre_patch(): # -> None:
        ...
    


