"""
This type stub file was generated by pyright.
"""

from .llama import *

torch_nn_functional_gelu = ...
def fast_geglu_inference(self, X): # -> Tensor | Any | None:
    ...

def GemmaDecoderLayer_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: Optional[bool] = ..., use_cache: Optional[bool] = ..., padding_mask: Optional[torch.LongTensor] = ..., *args, **kwargs): # -> tuple[Any | Tensor, ...] | tuple[Any | Tensor, Any] | tuple[Any | Tensor]:
    ...

def GemmaModel_fast_forward_inference(self, input_ids, past_key_values, position_ids, attention_mask=...): # -> BaseModelOutputWithPast:
    ...

class GemmaFixedRotaryEmbedding(torch.nn.Module):
    def __init__(self, dim=..., max_position_embeddings=..., base=..., device=..., config=...) -> None:
        ...
    
    def forward(self, x, position_ids=..., seq_len=...): # -> tuple[Any, Any]:
        ...
    
    def get_cached(self, seq_len=..., device_index=...): # -> tuple[None, None]:
        ...
    
    def extend_rope_embedding(self, x, seq_len): # -> None:
        ...
    


class GemmaFixedLinearScalingRotaryEmbedding(GemmaFixedRotaryEmbedding):
    """LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev"""
    def __init__(self, dim=..., max_position_embeddings=..., base=..., device=..., scaling_factor=..., config=...) -> None:
        ...
    


class FastGemmaModel(FastLlamaModel):
    @staticmethod
    def pre_patch(): # -> None:
        ...
    
    @staticmethod
    def post_patch(model, tokenizer): # -> tuple[Any, Any]:
        ...
    


