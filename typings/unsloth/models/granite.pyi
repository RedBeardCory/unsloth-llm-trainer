"""
This type stub file was generated by pyright.
"""

from .llama import *
from .llama import LlamaRotaryEmbedding
from .mistral import *

def GraniteAttention_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: bool = ..., use_cache: bool = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
    ...

def GraniteDecoderLayer_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: Optional[bool] = ..., use_cache: Optional[bool] = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs): # -> tuple[Tensor | Any, ...] | tuple[Tensor, ...] | tuple[Tensor, Any] | tuple[Tensor]:
    ...

KV_CACHE_INCREMENT = ...
torch_nn_functional_softmax = ...
torch_matmul = ...
torch_tanh = ...
def GraniteAttention_fast_forward_inference(self, hidden_states: torch.Tensor, past_key_value: Optional[Tuple[torch.Tensor]], position_ids, do_prefill=..., attention_mask=..., use_sliding_window=..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ...): # -> tuple[Tensor | Any | None, tuple[Tensor | Any, Tensor | Any]]:
    ...

def GraniteModel_fast_forward_inference(self, input_ids, past_key_values, position_ids, attention_mask=...): # -> BaseModelOutputWithPast:
    ...

class GraniteRotaryEmbedding(LlamaRotaryEmbedding):
    def __init__(self, config) -> None:
        ...
    


def patched_init(original_init): # -> Callable[..., None]:
    ...

class FastGraniteModel(FastLlamaModel):
    @staticmethod
    def pre_patch(): # -> None:
        ...
    
    @staticmethod
    def post_patch(model, tokenizer): # -> tuple[Any, Any]:
        ...
    


