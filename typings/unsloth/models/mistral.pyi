"""
This type stub file was generated by pyright.
"""

from .llama import *

def MistralAttention_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: bool = ..., use_cache: bool = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
    ...

def MistralForCausalLM_fast_forward(self, input_ids: torch.LongTensor = ..., causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_values: Optional[List[torch.FloatTensor]] = ..., inputs_embeds: Optional[torch.FloatTensor] = ..., labels: Optional[torch.LongTensor] = ..., use_cache: Optional[bool] = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., num_logits_to_keep: Optional[int] = ..., logits_to_keep: Optional[int] = ..., *args, **kwargs) -> Union[Tuple, CausalLMOutputWithPast]:
    ...

def patch_mistral_nemo_attention(function):
    ...

class FastMistralModel(FastLlamaModel):
    @staticmethod
    def pre_patch(): # -> None:
        ...
    
    @staticmethod
    def from_pretrained(model_name=..., max_seq_length=..., dtype=..., load_in_4bit=..., token=..., device_map=..., rope_scaling=..., fix_tokenizer=..., model_patcher=..., tokenizer_name=..., trust_remote_code=..., **kwargs):
        ...
    


