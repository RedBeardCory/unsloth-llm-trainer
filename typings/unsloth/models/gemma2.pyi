"""
This type stub file was generated by pyright.
"""

from .llama import *

if HAS_FLASH_ATTENTION_SOFTCAPPING:
    ...
def Gemma2Attention_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: bool = ..., use_cache: bool = ..., padding_mask: Optional[torch.LongTensor] = ..., *args, **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
    ...

def Gemma2DecoderLayer_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: Optional[bool] = ..., use_cache: Optional[bool] = ..., padding_mask: Optional[torch.LongTensor] = ..., *args, **kwargs): # -> tuple[Any | Tensor, ...] | tuple[Any | Tensor, Any] | tuple[Any | Tensor]:
    ...

KV_CACHE_INCREMENT = ...
torch_nn_functional_softmax = ...
torch_matmul = ...
torch_tanh = ...
def Gemma2Attention_fast_forward_inference(self, hidden_states: torch.Tensor, past_key_value: Optional[Tuple[torch.Tensor]], position_ids, do_prefill=..., attention_mask=..., use_sliding_window=...): # -> tuple[Tensor | Any | None, tuple[Tensor | Any, Tensor | Any]]:
    ...

def Gemma2Model_fast_forward_inference(self, input_ids, past_key_values, position_ids, attention_mask=...): # -> BaseModelOutputWithPast:
    ...

class FastGemma2Model(FastLlamaModel):
    @staticmethod
    def pre_patch(): # -> None:
        ...
    
    @staticmethod
    def post_patch(model, tokenizer): # -> tuple[Any, Any]:
        ...
    


