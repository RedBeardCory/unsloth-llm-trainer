"""
This type stub file was generated by pyright.
"""

import torch
import functools
import os
import logging
import transformers.generation.configuration_utils
from typing import Any, Callable, List, Optional, Tuple
from platform import system as platform_system
from dataclasses import dataclass
from unsloth_zoo.utils import Version
from ..device_type import DEVICE_COUNT, DEVICE_TYPE
from transformers.utils import is_openai_available
from xformers import __version__ as xformers_version
from peft import __version__ as peft_version
from torchao.core.config import AOBaseConfig

__version__ = ...
__all__ = ["SUPPORTS_BFLOAT16", "is_bfloat16_supported", "is_vLLM_available", "prepare_model_for_kbit_training", "xformers", "xformers_attention", "xformers_version", "__version__", "importlib_version", "HAS_FLASH_ATTENTION", "HAS_FLASH_ATTENTION_SOFTCAPPING", "USE_MODELSCOPE", "platform_system", "patch_tokenizer", "get_statistics", "Unsloth_Offloaded_Gradient_Checkpointer", "offload_to_disk", "offload_input_embeddings", "offload_output_embeddings", "unsloth_offloaded_gradient_checkpoint", "torch_compile_options", "patch_linear_scaling", "patch_llama_rope_scaling", "create_boolean_mask", "torch_amp_custom_fwd", "torch_amp_custom_bwd", "patch_gradient_accumulation_fix", "patch_compiling_bitsandbytes", "patch_regional_compilation", "patch_layernorm", "patch_torch_compile", "patch_model_and_tokenizer", "patch_unsloth_gradient_checkpointing", "unpatch_unsloth_gradient_checkpointing", "patch_gradient_checkpointing", "unpatch_gradient_checkpointing", "HAS_CUT_CROSS_ENTROPY", "EMPTY_LOGITS", "fused_linear_cross_entropy", "unsloth_fused_ce_loss", "patch_unsloth_smart_gradient_checkpointing", "unpatch_unsloth_smart_gradient_checkpointing", "patch_compiled_autograd", "process_vision_info", "unsloth_compile_transformers", "patch_fast_lora", "validate_loftq_config", "RaiseUninitialized", "fast_inference_setup", "patch_peft_fast_inference", "error_out_no_vllm", "dequantize_module_weight", "patch_hf_quantizer", "verify_fp8_support_if_applicable", "_get_inference_mode_context_manager"]
platform_system = ...
class HideLoggingMessage(logging.Filter):
    __slots__ = ...
    def __init__(self, text) -> None:
        ...
    
    def filter(self, x): # -> bool:
        ...
    


if os.environ.get("UNSLOTH_ENABLE_LOGGING", "0") != "1":
    ...
class _RaiseUninitialized(logging.Handler):
    def __init__(self) -> None:
        ...
    
    def emit(self, record): # -> None:
        ...
    


class RaiseUninitialized:
    def __init__(self) -> None:
        ...
    
    def remove(self): # -> None:
        ...
    


def extract_quant_model_param_count(model): # -> int:
    """
    Calculate quant model param count based on difference in param class. Returns int for param count.
    """
    ...

def get_model_param_count(model, trainable_only=...): # -> int:
    """
    Calculate model's total param count. If trainable_only is True then count only those requiring grads
    """
    ...

def patch_mistral_nemo_config(config):
    ...

model_architectures = ...
torch_version = ...
if DEVICE_TYPE in ("cuda", "hip"):
    ...
else:
    ...
if is_openai_available():
    ...
SUPPORTS_BFLOAT16 = ...
HAS_FLASH_ATTENTION = ...
HAS_FLASH_ATTENTION_SOFTCAPPING = ...
if DEVICE_TYPE == "cuda":
    ...
else:
    SUPPORTS_BFLOAT16 = ...
if (f"{major_version}.{minor_version}" in ("10.0", "11.0", "12.0")) and (Version(xformers_version) in (Version("0.0.32.post2"), )):
    ...
if False:
    ...
if Version(torch_version) < Version("2.2.0") and Version(xformers_version) >= Version("0.0.24"):
    ...
else:
    ...
xformers_attention = ...
if False:
    ...
if hasattr(transformers.generation.configuration_utils, "ALL_CACHE_IMPLEMENTATIONS"):
    ...
UNSLOTH_COMPILE_DEBUG = ...
UNSLOTH_COMPILE_MAXIMUM = ...
UNSLOTH_COMPILE_IGNORE_ERRORS = ...
@functools.lru_cache(None)
def is_big_gpu(index) -> bool:
    ...

torch_compile_options = ...
def torch_compile_kwargs(*args, **kwargs): # -> dict[str, bool | dict[str, bool]]:
    ...

def patch_regional_compilation(): # -> None:
    ...

def prepare_model_for_kbit_training(model: Any, use_gradient_checkpointing: Optional = ..., use_reentrant: Optional[bool] = ...) -> Any:
    ...

if Version(peft_version) < Version("0.12.0"):
    source = ...
    text = ...
    start = ...
    end = ...
    spaces = ...
    source = ...
    spaces = ...
    lines = ...
    source = ...
    source = ...
    source = ...
USE_MODELSCOPE = ...
if USE_MODELSCOPE:
    ...
@functools.lru_cache(1)
def has_internet(host=..., port=..., timeout=...): # -> bool:
    ...

def get_statistics(local_files_only=...): # -> None:
    ...

BitsAndBytesConfig__init__ = ...
BitsAndBytesConfig__init__ = ...
BitsAndBytesConfig__init__ = ...
length_spaces = ...
BitsAndBytesConfig__init__ = ...
BitsAndBytesConfig__init__ = ...
if DEVICE_COUNT == 1:
    ...
def move_to_device(target_device, *tensors): # -> tuple[Any, ...]:
    """
    Move multiple tensors to target device if they're not already there.

    Args:
        target_device: The target device to move tensors to
        *tensors: Variable number of tensors to potentially move

    Returns:
        tuple: The tensors on the target device (same objects if already on device, new if moved)
    """
    ...

def offload_to_disk(W, model, name, temporary_location: str = ...): # -> Any:
    ...

def offload_input_embeddings(model, temporary_location: str = ...): # -> None:
    ...

def offload_output_embeddings(model, temporary_location: str = ...): # -> None:
    ...

def is_bfloat16_supported(): # -> bool:
    ...

def is_vLLM_available(): # -> tuple[bool, str] | bool:
    ...

def patch_linear_scaling(model_name=..., rope_module=..., scaled_rope_module=..., attention_module=...): # -> tuple[None, None] | tuple[None, str] | tuple[str, str]:
    ...

def patch_llama_rope_scaling(model_name=..., rope_module=..., scaled_rope_module=..., extended_rope_module=..., attention_module=..., longrope_module=...): # -> tuple[None, None] | tuple[None, str] | tuple[str, str]:
    ...

def create_boolean_mask(n=..., sliding_window=...): # -> Tensor:
    ...

def test_mask_creation(): # -> None:
    ...

def patch_gradient_accumulation_fix(Trainer): # -> None:
    ...

def patch_tokenizer(model, tokenizer): # -> tuple[Any, Any]:
    ...

def patch_fast_lora(): # -> None:
    ...

def unsloth_compile_transformers(dtype, model_name, model_types, token=..., revision=..., trust_remote_code=..., sdpa_dynamic_mask=..., sdpa_bool_masks=..., sdpa_gqa_replace=..., sdpa_dynamic_compile=..., compile_attention=..., disable_causal_masks=..., compile_torch_modules=..., compile_custom_modules=..., compile_function_calls=..., fuse_lm_head=..., gradient_checkpointing=..., manual_replacements=..., fast_lora_forwards=..., fast_residual_stream=..., accurate_accumulation=..., epilogue_fusion=..., max_autotune=..., shape_padding=..., cudagraphs=..., debug=..., fullgraph=..., import_from_cache=..., disable=..., return_logits=..., unsloth_force_compile=...): # -> tuple[Any, Literal[False]] | tuple[list[Any], Literal[False]] | tuple[list[Any], bool] | None:
    ...

LOGITS_ERROR_STRING = ...
def raise_logits_error(*args, **kwargs):
    ...

def return_none(*args, **kwargs): # -> None:
    ...

class EmptyLogits:
    def __init__(self) -> None:
        ...
    
    def raise_getattr_error(self, attr): # -> Callable[..., None] | Callable[..., Never]:
        ...
    
    __getitem__ = ...
    __getattr__ = ...
    def __repr__(self): # -> LiteralString:
        ...
    
    def __str__(self) -> str:
        ...
    


EMPTY_LOGITS = ...
functions = ...
def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, model): # -> LoftQConfig | dict[Any, Any]:
    ...

def fast_inference_setup(model_name, model_config): # -> tuple[bool, Any]:
    ...

def patch_peft_fast_inference(model): # -> None:
    ...

def error_out_no_vllm(*args, **kwargs):
    ...

@dataclass
class TorchAOConfig:
    qat_scheme: Optional[str] = ...
    base_config_and_filter_fns: List[Tuple[AOBaseConfig, Optional[Callable[[torch.nn.Module, str], bool]]]] = ...
    prequantization_transform: Optional[Callable[[torch.nn.Module], None]] = ...


def patch_hf_quantizer(): # -> None:
    ...

def verify_fp8_support_if_applicable(model_config): # -> None:
    ...

