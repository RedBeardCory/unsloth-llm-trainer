"""
This type stub file was generated by pyright.
"""

import torch
from typing import List, Optional, Tuple, Union
from ._utils import *
from transformers import __version__ as transformers_version
from ..device_type import DEVICE_TYPE
from transformers.models.llama.modeling_llama import BaseModelOutputWithPast
from ..kernels import *
from ..tokenizer_utils import *

transformers_version = ...
IS_ATTENTION_REFACTOR = ...
if HAS_FLASH_ATTENTION:
    ...
HAS_XFORMERS = ...
BlockDiagonalCausalMask = ...
if DEVICE_TYPE == "xpu":
    clean_gpu_cache = ...
    get_current_device = ...
else:
    clean_gpu_cache = ...
    get_current_device = ...
def original_apply_qkv(self, X): # -> tuple[Any, Any, Any]:
    ...

def original_apply_o(self, X):
    ...

KV_CACHE_INCREMENT = ...
torch_nn_functional_softmax = ...
SDPA_HAS_GQA = ...
def fix_prepare_inputs_for_generation(module): # -> None:
    ...

torch_matmul = ...
def LlamaAttention_fast_forward_inference(self, hidden_states: torch.Tensor, past_key_value: Optional[Tuple[torch.Tensor]], position_ids, do_prefill=..., attention_mask=...): # -> tuple[Tensor | Any | None, tuple[Tensor | Any, Tensor | Any]]:
    """
    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
    Fast inference using KV cache.
    QK^T can be computed in 4 chunks

    [Q, q] @ [K, k].T where q, k are the new tokens.
    [QK^T, Qk^T]
    [qK^T, qk^T]

    Since the attention mask wipes Qk^T, we just get
    [QK^T,    0]
    [qK^T, qk^T]

    Since softmax is row-wise, we get
    softmax([QK^T,    0])
    softmax([qK^T, qk^T])

    We then multiply by   [V]
                          [v]
    softmax([QK^T,    0]) [softmax(QK^T)V] *
    softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]

    But notice * [softmax(QK^T)V] is just the last attention.
    We just need to compute the last final row.

    This means we can pass in a row of Q, but we need to
    remember K and V, which are called the KV cache.
    """
    ...

torch_nn_functional_silu = ...
def fast_swiglu_inference(self, X, temp_gate=..., temp_up=..., gate_multiplier=..., down_multiplier=...): # -> Tensor | Any | None:
    ...

torch_square = ...
torch_mean = ...
def fast_rms_layernorm_inference(self, X, XX=..., XX2=..., variance=...):
    ...

def fast_rms_layernorm_inference_gemma(self, X, out_weight=...):
    ...

@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)
def fast_layernorm_compiled(layernorm, X):
    ...

def LlamaAttention_fast_forward(self, hidden_states: torch.Tensor, causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: bool = ..., use_cache: bool = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
    ...

def LlamaDecoderLayer_fast_forward(self, hidden_states: torch.Tensor, causal_mask=..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_value: Optional[Tuple[torch.Tensor]] = ..., output_attentions: Optional[bool] = ..., use_cache: Optional[bool] = ..., padding_mask: Optional[torch.LongTensor] = ..., position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = ..., *args, **kwargs) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
    """
    Args:
        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
        attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
            (see `past_key_values`).
        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
    """
    ...

__DTYPE_MAP = ...
def LlamaModel_fast_forward(self, input_ids: Optional[torch.LongTensor] = ..., causal_mask: Optional[BlockDiagonalCausalMask] = ..., attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.LongTensor] = ..., past_key_values: Optional[List[torch.FloatTensor]] = ..., inputs_embeds: Optional[torch.FloatTensor] = ..., use_cache: Optional[bool] = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., *args, **kwargs) -> Union[Tuple, BaseModelOutputWithPast]:
    ...

LlamaModel_fast_forward_inference = ...
def CausalLM_fast_forward(fast_forward_inference): # -> Callable[..., Tuple[Any, ...] | CausalLMOutputWithPast]:
    ...

@torch._disable_dynamo
def PeftModel_fast_forward(self, input_ids=..., causal_mask=..., attention_mask=..., inputs_embeds=..., labels=..., output_attentions=..., output_hidden_states=..., return_dict=..., task_ids=..., num_logits_to_keep=..., logits_to_keep=..., **kwargs):
    ...

class LlamaRotaryEmbedding(torch.nn.Module):
    def __init__(self, dim=..., max_position_embeddings=..., base=..., device=..., config=...) -> None:
        ...
    
    def forward(self, x, position_ids=..., seq_len=...): # -> tuple[Any, Any]:
        ...
    
    def get_cached(self, seq_len=..., device_index=...): # -> tuple[None, None]:
        ...
    
    def extend_rope_embedding(self, x, seq_len): # -> None:
        ...
    


class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
    """LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev"""
    def __init__(self, dim=..., max_position_embeddings=..., base=..., device=..., scaling_factor=..., config=...) -> None:
        ...
    


class LlamaExtendedRotaryEmbedding(torch.nn.Module):
    def __init__(self, dim=..., max_position_embeddings=..., base=..., device=..., config=...) -> None:
        ...
    
    def forward(self, x, position_ids=..., seq_len=...): # -> tuple[Any, Any]:
        ...
    
    def get_cached(self, seq_len=..., device_index=...): # -> tuple[None, None]:
        ...
    
    def extend_rope_embedding(self, x, seq_len): # -> None:
        ...
    
    def apply_scaling(self, freqs: torch.Tensor): # -> Tensor:
        ...
    


class LongRopeRotaryEmbedding(torch.nn.Module):
    def __init__(self, dim=..., max_position_embeddings=..., original_max_position_embeddings=..., base=..., short_factor=..., long_factor=..., device=..., config=...) -> None:
        ...
    
    def forward(self, x, position_ids=..., seq_len=...): # -> tuple[Any, Any]:
        ...
    
    def get_cached(self, seq_len=..., device_index=...): # -> tuple[None, None]:
        ...
    
    def extend_rope_embedding(self, x, seq_len): # -> None:
        ...
    


def unsloth_fast_generate(self, *args, **kwargs):
    ...

class FastLlamaModel:
    @staticmethod
    def pre_patch(): # -> None:
        ...
    
    @staticmethod
    def from_pretrained(model_name=..., max_seq_length=..., dtype=..., load_in_4bit=..., token=..., device_map=..., rope_scaling=..., fix_tokenizer=..., model_patcher=..., tokenizer_name=..., trust_remote_code=..., revision=..., fast_inference=..., gpu_memory_utilization=..., float8_kv_cache=..., random_state=..., max_lora_rank=..., disable_log_stats=..., unsloth_vllm_standby=..., num_labels=..., qat_scheme=..., **kwargs):
        ...
    
    @staticmethod
    def post_patch(model, tokenizer): # -> tuple[Any, Any]:
        ...
    
    @staticmethod
    def get_peft_model(model, r=..., target_modules=..., lora_alpha=..., lora_dropout=..., bias=..., layers_to_transform=..., layers_pattern=..., use_gradient_checkpointing=..., random_state=..., max_seq_length=..., use_rslora=..., modules_to_save=..., init_lora_weights=..., loftq_config=..., temporary_location=..., qat_scheme=..., **kwargs):
        ...
    
    @staticmethod
    def patch_peft_model(model, use_gradient_checkpointing=...):
        ...
    
    @staticmethod
    def for_inference(model):
        ...
    
    @staticmethod
    def for_training(model, use_gradient_checkpointing=...):
        ...
    


