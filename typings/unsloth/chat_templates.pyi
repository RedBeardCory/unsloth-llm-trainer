"""
This type stub file was generated by pyright.
"""

from transformers import StoppingCriteria
from .tokenizer_utils import *

__all__ = ["get_chat_template", "test_chat_templates", "test_hf_gguf_equivalence", "remove_special_tokens", "to_sharegpt", "standardize_sharegpt", "standardize_data_formats", "apply_chat_template", "train_on_responses_only", "test_construct_chat_template"]
standardize_sharegpt = ...
CHAT_TEMPLATES = ...
DEFAULT_SYSTEM_MESSAGE = ...
unsloth_template = ...
unsloth_ollama = ...
unsloth_eos_token = ...
zephyr_template = ...
zephyr_ollama = ...
zephyr_eos_token = ...
chatml_template = ...
chatml_ollama = ...
chatml_eos_token = ...
mistral_template = ...
mistral_ollama = ...
mistral_eos_token = ...
llama_template = ...
llama_ollama = ...
llama_eos_token = ...
vicuna_template = ...
vicuna_ollama = ...
vicuna_eos_token = ...
vicuna_old_template = ...
vicuna_old_ollama = ...
vicuna_old_eos_token = ...
alpaca_template = ...
alpaca_ollama = ...
alpaca_eos_token = ...
gemma_template = ...
gemma_ollama = ...
gemma_eos_token = ...
gemma_chatml_template = ...
gemma_chatml_ollama = ...
gemma_chatml_eos_token = ...
gemma2_template = ...
gemma2_ollama = ...
gemma2_eos_token = ...
gemma2_chatml_template = ...
gemma2_chatml_ollama = ...
gemma2_chatml_eos_token = ...
llama3_template = ...
llama3_ollama = ...
llama3_template_eos_token = ...
phi3_template = ...
phi3_ollama = ...
phi3_template_eos_token = ...
llama31_template = ...
llama31_ollama = ...
llama31_template_eos_token = ...
qwen25_template = ...
qwen25_ollama = ...
qwen25_template_eos_token = ...
qwen25_default_system_message = ...
phi4_template = ...
_phi4_ollama_template = ...
phi4_ollama = ...
phi4_template_eos_token = ...
gemma3_template = ...
gemma3_ollama = ...
gemma3_template_eos_token = ...
qwen3_template = ...
qwen3_ollama = ...
qwen3_template_eos_token = ...
gemma3n_template = ...
gemma3n_ollama = ...
gemma3n_template_eos_token = ...
gptoss_template = ...
gptoss_ollama = ...
gptoss_template_template_eos_token = ...
qwen3_instruct_template = ...
qwen3_ollama = ...
qwen3_template_eos_token = ...
qwen3_thinking_template = ...
liquid_lfm2_template = ...
liquid_lfm2_template_eos_token = ...
starling_template = ...
starling_ollama = ...
starling_template_eos_token = ...
yi_chat_template = ...
yi_chat_ollama = ...
yi_chat_template_eos_token = ...
def get_chat_template(tokenizer, chat_template=..., mapping=..., map_eos_token=..., system_message=...):
    ...

def remove_special_tokens(tokenizer, prompt):
    ...

def to_sharegpt(dataset, merged_prompt=..., merged_column_name=..., output_column_name=..., remove_unused_columns=..., conversation_extension=..., random_state=...):
    """
    Converts a dataset to ShareGPT style.
    ShareGPT requires only 1 input and 1 output field.
    This means one has to merge multiple columns into 1 for 1 input field.
    Use `conversation_extension` to increase the length of each conversation by randomnly
    selecting a few and packing them into 1.

    merged_prompt = "",                 Prompt to merge columns into 1 input
    merged_column_name = "instruction", Final column name for the input  field
    output_column_name = "output",      Final column name for the output field
    remove_unused_columns = True,
    conversation_extension = 1,         Automatically combines `conversation_extension` convos into 1
    random_state = 3407,
    """
    ...

def get_ollama_eos_tokens(tokenizer, extra_eos_tokens=...): # -> list[Any]:
    ...

def construct_chat_template(tokenizer=..., chat_template=..., default_system_message=..., extra_eos_tokens=...):
    """
    Creates a Ollama modelfile and a HF Jinja template from a custom
    template. You must provide 2x examples of an input & output.
    There is an optional system message as well.

    You must use {INPUT}, {OUTPUT} twice, and {SYSTEM} is optional.
    """
    ...

def test_construct_chat_template(): # -> None:
    ...

def apply_chat_template(dataset, tokenizer=..., chat_template=..., default_system_message=..., extra_eos_tokens=...):
    """
    Creates a Ollama modelfile and a HF Jinja template from a custom
    template. You must provide 2x examples of an input & output.
    There is an optional system message as well.

    You must use {INPUT}, {OUTPUT} twice, and {SYSTEM} is optional.
    """
    ...

def create_stopping_criteria(tokenizer, stop_word=...): # -> StoppingCriteriaList:
    class StoppingCriteriaSub(StoppingCriteria):
        ...
    
    

def test_chat_templates(): # -> None:
    ...

def test_hf_gguf_equivalence(tokenizer, gguf_model=...): # -> Literal[True]:
    """
        Carefully checks the output of GGUF's tokenization and HF.
        Can catch all tokenization bugs.
    """
    ...

