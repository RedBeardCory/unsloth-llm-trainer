"""
This type stub file was generated by pyright.
"""

from trl.trainer.sft_trainer import *
from transformers.trainer import *

__all__ = ["load_correct_tokenizer", "fix_sentencepiece_tokenizer", "check_tokenizer", "add_new_tokens", "fix_sentencepiece_gguf"]
IGNORED_TOKENIZER_CHECKING = ...
IGNORED_TOKENIZER_NAMES = ...
IGNORED_TOKENIZER_NAMES = ...
keynames = ...
IS_COLAB_ENVIRONMENT = ...
IS_KAGGLE_ENVIRONMENT = ...
KAGGLE_TMP = ...
def try_fix_tokenizer(tokenizer, prepend=...):
    ...

def get_sorted_dict(dictionary): # -> dict[Any, Any]:
    ...

def convert_to_fast_tokenizer(slow_tokenizer, temporary_location=...):
    ...

mistral_template = ...
llama_template = ...
def assert_same_tokenization(slow_tokenizer, fast_tokenizer): # -> bool:
    ...

def fix_sentencepiece_tokenizer(old_tokenizer, new_tokenizer, token_mapping, temporary_location=...):
    ...

def fix_sentencepiece_gguf(saved_location): # -> None:
    """
    Fixes sentencepiece tokenizers which did not extend the vocabulary with
    user defined tokens.
    Inspiration from https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py
    """
    class SentencePieceTokenTypes(IntEnum):
        ...
    
    

def load_correct_tokenizer(tokenizer_name, model_max_length=..., padding_side=..., token=..., trust_remote_code=..., cache_dir=..., fix_tokenizer=...):
    ...

def fix_chat_template(tokenizer): # -> Any | None:
    ...

def check_tokenizer(model, tokenizer, model_name=..., model_max_length=..., padding_side=..., token=..., _reload=...):
    ...

def patch_sft_trainer_tokenizer(): # -> None:
    """
    Patches the trainer with changes
    """
    ...

