"""
This type stub file was generated by pyright.
"""

import torch
import functools
from functools import lru_cache
from torch.nn.attention.flex_attention import flex_attention as _flex_attention

torch_compile_options = ...
_flex_attention = ...
HAS_FLEX_ATTENTION = ...
if not HAS_FLEX_ATTENTION:
    @torch.compile(fullgraph=True, dynamic=True, options=torch_compile_options)
    def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len): # -> Tensor:
        ...
    
    create_flex_attention_causal_mask = ...
    create_flex_attention_sliding_window_mask = ...
else:
    def generate_tanh_softcap(t): # -> Callable[..., Any]:
        ...
    
    def causal_masker(b, h, q_idx, kv_idx):
        ...
    
    @functools.lru_cache
    def sliding_window_masker(size=...): # -> Callable[..., Any]:
        ...
    
    @functools.lru_cache
    def create_block_mask(mask, n=...): # -> BlockMask:
        ...
    
    def create_flex_attention_causal_mask(max_seq_length=...): # -> BlockMask:
        ...
    
    def create_flex_attention_sliding_window_mask(max_seq_length=..., sliding_window=...): # -> BlockMask:
        ...
    
    @functools.lru_cache
    def flex_attention(s, t): # -> partial[Tensor | tuple[Tensor, Tensor] | tuple[Tensor, AuxOutput]]:
        ...
    
    def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len): # -> Tensor:
        ...
    
torch_matmul = ...
torch_tanh = ...
torch_nn_functional_softmax = ...
def slow_inference_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len): # -> Tensor:
    ...

