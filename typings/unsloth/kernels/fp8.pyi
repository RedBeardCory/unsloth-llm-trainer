"""
This type stub file was generated by pyright.
"""

import os
import torch
import triton
import triton.language as tl
import fbgemm_gpu
from unsloth_zoo.utils import Version
from unsloth_zoo.temporary_patches.common import torch_compile
from transformers.integrations.finegrained_fp8 import FP8Linear
from transformers.integrations.fbgemm_fp8 import FbgemmFp8Linear

torch_matmul = ...
@triton.jit
def weight_dequant_kernel(x_ptr, s_ptr, y_ptr, M, N, BLOCK_SIZE: tl.constexpr):
    ...

def weight_dequant_block(x: torch.Tensor, s: torch.Tensor, block_size: int = ..., dtype=...) -> torch.Tensor:
    ...

def weight_dequant(x: torch.Tensor, s: torch.Tensor, dtype=...): # -> Tensor:
    ...

@triton.jit
def act_quant_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr):
    ...

def act_quant(x: torch.Tensor, block_size: int = ...) -> tuple[torch.Tensor, torch.Tensor]:
    ...

def w8a8_block_fp8_matmul_triton(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: list[int], output_dtype: torch.dtype = ...) -> torch.Tensor:
    """This function performs matrix multiplication with block-wise
    quantization.
    It takes two input tensors `A` and `B` with scales `As` and `Bs`.
    The output is returned in the specified `output_dtype`.
    Args:
        A: The input tensor, e.g., activation.
        B: The input tensor, e.g., weight.
        As: The per-token-group quantization scale for `A`.
        Bs: The per-block quantization scale for `B`.
        block_size: The block size for per-block quantization. It should
        be 2-dim, e.g., [128, 128].
        output_dytpe: The dtype of the returned tensor.
    Returns:
        torch.Tensor: The result of matmul.
    """
    ...

def torchao_block_matmul(act_q: torch.Tensor, weight_q: torch.Tensor, act_scale: torch.Tensor, weight_scale: torch.Tensor, block_size: tuple[int, int], output_dtype: torch.dtype = ...): # -> Tensor:
    ...

fp8_block_matmul = ...
class FP8BlockQuantLinear(torch.autograd.Function):
    @staticmethod
    def forward(ctx, X, weight, weight_scale): # -> Tensor:
        ...
    
    @staticmethod
    def backward(ctx, grad_output): # -> tuple[Tensor, None, None]:
        ...
    


@torch_compile
def fp8_torch_block_quant_forward(X, weight, weight_scale): # -> Any | None:
    ...

class FbgemmFp8Linear_matmul(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, weight, weight_scale, bias=...): # -> Any | Tensor:
        ...
    
    @staticmethod
    def backward(ctx, grad_output): # -> tuple[Tensor, None, None, None, None]:
        ...
    


@torch_compile
def fbgemm_fp8_linear(X, weight, weight_scale, bias=...): # -> Any | None:
    ...

class FP8_fbgemm_block_linear(torch.autograd.Function):
    @staticmethod
    def forward(ctx, X, weight, weight_scale, bias=...): # -> Any:
        ...
    
    @staticmethod
    def backward(ctx, grad_output): # -> tuple[Tensor, None, None, None, None]:
        ...
    


@torch_compile
def fp8_fbgemm_block_linear(X, weight, weight_scale, bias=...): # -> Any | None:
    ...

def test_has_fbgemm(): # -> bool:
    ...

fp8_block_quant_linear = ...
if "UNSLOTH_HAS_FBGEMM" not in os.environ:
    ...
if Version(fbgemm_gpu.__version__) >= Version("1.4.0"):
    ...
@torch_compile
def fp8_linear(X, weight, weight_scale, bias=...): # -> Any | None:
    ...

def module_forward_patch(forward_function, scale_attr=...): # -> Callable[..., Any]:
    ...

if FbgemmFp8Linear is not None:
    ...
if FP8Linear is not None:
    ...
