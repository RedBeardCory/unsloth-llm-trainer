"""
This type stub file was generated by pyright.
"""

import importlib
import triton
import ctypes
import functools
import torch
from ..device_type import DEVICE_COUNT, DEVICE_TYPE
from unsloth_zoo.utils import Version
from torchao.quantization import Float8Tensor

MAX_FUSED_SIZE: int = ...
next_power_of_2 = ...
torch_Tensor = torch.Tensor
if DEVICE_TYPE == "xpu" and Version(torch.__version__) < Version("2.6.0"):
    ...
if Version(torch.__version__) < Version("2.4.0"):
    torch_amp_custom_fwd = ...
    torch_amp_custom_bwd = ...
else:
    torch_amp_custom_fwd = ...
    torch_amp_custom_bwd = ...
if DEVICE_TYPE == "xpu":
    torch_amp_custom_fwd = ...
    torch_amp_custom_bwd = ...
if Version(triton.__version__) >= Version("3.0.0"):
    triton_cast = ...
else:
    triton_tanh = ...
    @triton.jit
    def triton_cast(x, dtype):
        ...
    
@functools.lru_cache(1)
def is_cdna(): # -> bool:
    ...

def calculate_settings(n: int) -> (int, int):
    ...

HAS_CUDA_STREAM = ...
HAS_CUDA_STREAM = ...
get_ptr = ...
if DEVICE_TYPE == "xpu":
    HAS_XPU_STREAM = ...
if DEVICE_COUNT > 1:
    ...
else:
    def torch_gpu_device(device): # -> nullcontext[None]:
        ...
    
if DEVICE_TYPE == "xpu":
    _gpu_getCurrentRawStream = ...
else:
    _gpu_getCurrentRawStream = ...
c_void_p = ctypes.c_void_p
if DEVICE_TYPE == "xpu":
    _XPU_STREAMS = ...
    XPU_STREAMS = ...
    WEIGHT_BUFFERS = ...
    ABSMAX_BUFFERS = ...
    XPU_STREAMS = ...
else:
    _CUDA_STREAMS = ...
    CUDA_STREAMS = ...
    WEIGHT_BUFFERS = ...
    ABSMAX_BUFFERS = ...
    CUDA_STREAMS = ...
ctypes_c_int = ctypes.c_int
ctypes_c_int32 = ctypes.c_int32
cdequantize_blockwise_fp32 = ...
cdequantize_blockwise_fp16_nf4 = ...
cdequantize_blockwise_bf16_nf4 = ...
if DEVICE_TYPE == "xpu":
    cgemm_4bit_inference_naive_fp16 = ...
    cgemm_4bit_inference_naive_bf16 = ...
else:
    cgemm_4bit_inference_naive_fp16 = ...
    cgemm_4bit_inference_naive_bf16 = ...
torch_device_stream = ...
torch_mm = ...
torch_mv = ...
torch_matmul = ...
torch_addmm = ...
torch_empty = ...
torch_float32 = ...
torch_float16 = ...
torch_bfloat16 = ...
if importlib.util.find_spec("torchao") is not None:
    ...
else:
    Float8Tensor = ...
def QUANT_STATE(W): # -> Any | None:
    ...

def get_lora_parameters(proj): # -> tuple[Any, Any | None, None, None, None] | tuple[Any, Any | None, Any, Any, Any]:
    """
    Return a 5-tuple of (weight, weight quant_state, lora A, lora B, and lora scale).
    If QAT is enabled, additionally fake quantize the base layer and lora weights.
    """
    ...

def get_lora_parameters_bias(proj): # -> tuple[Any, Any | None, None, None, None, Any] | tuple[Any, Any | None, Any, Any, Any, Any]:
    ...

if DEVICE_TYPE == "xpu" and HAS_XPU_STREAM:
    @torch.inference_mode
    def fast_dequantize(W, quant_state=..., out=..., use_global_buffer=...): # -> Tensor:
        ...
    
else:
    @torch.inference_mode
    def fast_dequantize(W, quant_state=..., out=..., use_global_buffer=...): # -> Tensor:
        ...
    
    @torch.inference_mode
    def fast_dequantize(W, quant_state=..., out=..., use_global_buffer=...): # -> Tensor:
        ...
    
if DEVICE_TYPE == "xpu" and HAS_XPU_STREAM:
    def fast_gemv(X, W, quant_state, out=...): # -> Tensor:
        ...
    
else:
    def fast_gemv(X, W, quant_state, out=...): # -> Tensor:
        ...
    
    def fast_gemv(X, W, quant_state, out=...): # -> Tensor:
        ...
    
def fast_linear_forward(proj, X, temp_lora=..., out=...): # -> Tensor | Any | None:
    ...

def matmul_lora(X, W, W_quant, A, B, s, out=...): # -> Tensor | Any | None:
    ...

