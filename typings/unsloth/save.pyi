"""
This type stub file was generated by pyright.
"""

import torch
import os
from typing import Callable, List, Optional, Union

__all__ = ["print_quantization_methods", "unsloth_save_model", "save_to_gguf", "patch_saving_functions", "create_huggingface_repo"]
LLAMA_CPP_TARGETS = ...
keynames = ...
IS_COLAB_ENVIRONMENT = ...
IS_KAGGLE_ENVIRONMENT = ...
KAGGLE_TMP = ...
LLAMA_WEIGHTS = ...
LLAMA_LAYERNORMS = ...
ALLOWED_QUANTS = ...
def print_quantization_methods(): # -> None:
    ...

def check_if_sentencepiece_model(model, temporary_location=...): # -> bool:
    ...

def fast_save_pickle(shard, name): # -> None:
    ...

@torch.inference_mode
def unsloth_save_model(model, tokenizer, save_directory: Union[str, os.PathLike], save_method: str = ..., push_to_hub: bool = ..., token: Optional[Union[str, bool]] = ..., is_main_process: bool = ..., state_dict: Optional[dict] = ..., save_function: Callable = ..., max_shard_size: Union[int, str] = ..., safe_serialization: bool = ..., variant: Optional[str] = ..., save_peft_format: bool = ..., use_temp_dir: Optional[bool] = ..., commit_message: Optional[str] = ..., private: Optional[bool] = ..., create_pr: bool = ..., revision: str = ..., commit_description: str = ..., tags: List[str] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> tuple[str | PathLike[Any], None] | tuple[str | Any | PathLike[Any], str | Any | None]:
    ...

def install_llama_cpp_clone_non_blocking(): # -> Popen[bytes]:
    ...

def install_llama_cpp_make_non_blocking(): # -> tuple[Popen[bytes], bool]:
    ...

def install_python_non_blocking(packages=...): # -> Popen[bytes]:
    ...

def try_execute(commands, force_complete=...): # -> Literal['CMAKE'] | None:
    ...

def install_llama_cpp_old(version=...): # -> None:
    ...

def install_llama_cpp_blocking(use_cuda=...): # -> None:
    ...

def get_executable(executables): # -> str | None:
    ...

def save_to_gguf(model_name: str, model_type: str, model_dtype: str, is_sentencepiece: bool = ..., model_directory: str = ..., quantization_method=..., first_conversion: str = ..., is_vlm: bool = ..., is_gpt_oss: bool = ...): # -> tuple[Any, bool, Any | bool]:
    """
    Orchestrates the complete GGUF conversion process.
    Handles installation, conversion, and quantization.
    """
    ...

def unsloth_save_pretrained_merged(self, save_directory: Union[str, os.PathLike], tokenizer=..., save_method: str = ..., push_to_hub: bool = ..., token: Optional[Union[str, bool]] = ..., is_main_process: bool = ..., state_dict: Optional[dict] = ..., save_function: Callable = ..., max_shard_size: Union[int, str] = ..., safe_serialization: bool = ..., variant: Optional[str] = ..., save_peft_format: bool = ..., tags: List[str] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> None:
    """
    Same as .save_pretrained(...) except 4bit weights are auto
    converted to float16 with as few overhead as possible.

    Choose for `save_method` to be either:
    1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
    2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
    3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
    """
    ...

def unsloth_push_to_hub_merged(self, repo_id: str, tokenizer=..., save_method: str = ..., use_temp_dir: Optional[bool] = ..., commit_message: Optional[str] = ..., private: Optional[bool] = ..., token: Union[bool, str, None] = ..., max_shard_size: Union[int, str, None] = ..., create_pr: bool = ..., safe_serialization: bool = ..., revision: str = ..., commit_description: str = ..., tags: Optional[List[str]] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> None:
    """
    Same as .push_to_hub(...) except 4bit weights are auto
    converted to float16 with as few overhead as possible.

    Choose for `save_method` to be either:
    1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
    2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
    3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
    """
    ...

MODEL_CARD = ...
def create_huggingface_repo(model, save_directory, token=..., private=...): # -> tuple[str | Any, HfApi]:
    ...

def upload_to_huggingface(model, save_directory, token, method, extra=..., file_location=..., old_username=..., private=..., create_config=...): # -> str:
    ...

def fix_tokenizer_bos_token(tokenizer): # -> tuple[bool, Any | None]:
    ...

def create_ollama_modelfile(tokenizer, base_model_name, model_location): # -> None:
    """
    Creates an Ollama Modelfile.
    Use ollama.create(model = "new_ollama_model", modelfile = modelfile)
    """
    ...

def create_ollama_model(username: str, model_name: str, tag: str, modelfile_path: str): # -> Literal['Ollama Request Timeout'] | None:
    ...

def push_to_ollama_hub(username: str, model_name: str, tag: str): # -> Literal['Ollama Request Timeout'] | None:
    ...

def push_to_ollama(tokenizer, gguf_location, username: str, model_name: str, tag: str): # -> None:
    ...

def unsloth_save_pretrained_gguf(self, save_directory: Union[str, os.PathLike], tokenizer=..., quantization_method=..., first_conversion: str = ..., push_to_hub: bool = ..., token: Optional[Union[str, bool]] = ..., private: Optional[bool] = ..., is_main_process: bool = ..., state_dict: Optional[dict] = ..., save_function: Callable = ..., max_shard_size: Union[int, str] = ..., safe_serialization: bool = ..., variant: Optional[str] = ..., save_peft_format: bool = ..., tags: List[str] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> dict[str, str | PathLike[Any] | Any | bool | None]:
    """
    Same as .save_pretrained(...) except 4bit weights are auto
    converted to float16 then converted to GGUF / llama.cpp format.

    Choose for `quantization_method` to be:
    "not_quantized"  : "Recommended. Fast conversion. Slow inference, big files.",
    "fast_quantized" : "Recommended. Fast conversion. OK inference, OK file size.",
    "quantized"      : "Recommended. Slow conversion. Fast inference, small files.",
    "f32"     : "Not recommended. Retains 100% accuracy, but super slow and memory hungry.",
    "f16"     : "Fastest conversion + retains 100% accuracy. Slow and memory hungry.",
    "q8_0"    : "Fast conversion. High resource use, but generally acceptable.",
    "q4_k_m"  : "Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K",
    "q5_k_m"  : "Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K",
    "q2_k"    : "Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.",
    "q3_k_l"  : "Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K",
    "q3_k_m"  : "Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K",
    "q3_k_s"  : "Uses Q3_K for all tensors",
    "q4_0"    : "Original quant method, 4-bit.",
    "q4_1"    : "Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.",
    "q4_k_s"  : "Uses Q4_K for all tensors",
    "q4_k"    : "alias for q4_k_m",
    "q5_k"    : "alias for q5_k_m",
    "q5_0"    : "Higher accuracy, higher resource usage and slower inference.",
    "q5_1"    : "Even higher accuracy, resource usage and slower inference.",
    "q5_k_s"  : "Uses Q5_K for all tensors",
    "q6_k"    : "Uses Q8_K for all tensors",
    "iq2_xxs" : "2.06 bpw quantization",
    "iq2_xs"  : "2.31 bpw quantization",
    "iq3_xxs" : "3.06 bpw quantization",
    "q3_k_xs" : "3-bit extra small quantization",
    """
    ...

def unsloth_push_to_hub_gguf(self, repo_id: str, tokenizer=..., quantization_method=..., first_conversion: str = ..., use_temp_dir: Optional[bool] = ..., commit_message: Optional[str] = ..., private: Optional[bool] = ..., token: Union[bool, str, None] = ..., max_shard_size: Union[int, str, None] = ..., create_pr: bool = ..., safe_serialization: bool = ..., revision: str = ..., commit_description: str = ..., tags: Optional[List[str]] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> str:
    """
    Same as .push_to_hub(...) except 4bit weights are auto
    converted to float16 then converted to GGUF / llama.cpp format.

    Choose for `quantization_method` to be:
    "not_quantized"  : "Recommended. Fast conversion. Slow inference, big files.",
    "fast_quantized" : "Recommended. Fast conversion. OK inference, OK file size.",
    "quantized"      : "Recommended. Slow conversion. Fast inference, small files.",
    "f32"     : "Not recommended. Retains 100% accuracy, but super slow and memory hungry.",
    "f16"     : "Fastest conversion + retains 100% accuracy. Slow and memory hungry.",
    "q8_0"    : "Fast conversion. High resource use, but generally acceptable.",
    "q4_k_m"  : "Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K",
    "q5_k_m"  : "Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K",
    "q2_k"    : "Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.",
    "q3_k_l"  : "Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K",
    "q3_k_m"  : "Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K",
    "q3_k_s"  : "Uses Q3_K for all tensors",
    "q4_0"    : "Original quant method, 4-bit.",
    "q4_1"    : "Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.",
    "q4_k_s"  : "Uses Q4_K for all tensors",
    "q5_0"    : "Higher accuracy, higher resource usage and slower inference.",
    "q5_1"    : "Even higher accuracy, resource usage and slower inference.",
    "q5_k_s"  : "Uses Q5_K for all tensors",
    "q6_k"    : "Uses Q8_K for all tensors",
    """
    ...

def save_lora_to_custom_dir(model, tokenizer, save_directory): # -> None:
    ...

def unsloth_convert_lora_to_ggml_and_push_to_hub(self, tokenizer, repo_id: str, use_temp_dir: Optional[bool] = ..., commit_message: Optional[str] = ..., private: Optional[bool] = ..., token: Union[bool, str, None] = ..., create_pr: bool = ..., revision: str = ..., commit_description: str = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> None:
    ...

def unsloth_convert_lora_to_ggml_and_save_locally(self, save_directory: str, tokenizer, temporary_location: str = ..., maximum_memory_usage: float = ...): # -> None:
    ...

@torch.inference_mode
def save_to_gguf_generic(model, save_directory, tokenizer, quantization_method=..., quantization_type=..., repo_id=..., token=...):
    ...

@torch.inference_mode
def unsloth_generic_save(model, tokenizer, save_directory: Union[str, os.PathLike] = ..., save_method: str = ..., push_to_hub: bool = ..., token: Optional[Union[str, bool]] = ..., is_main_process: bool = ..., state_dict: Optional[dict] = ..., save_function: Callable = ..., max_shard_size: Union[int, str] = ..., safe_serialization: bool = ..., variant: Optional[str] = ..., save_peft_format: bool = ..., use_temp_dir: Optional[bool] = ..., commit_message: Optional[str] = ..., private: Optional[bool] = ..., create_pr: bool = ..., revision: str = ..., commit_description: str = ..., tags: List[str] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> None:
    ...

def unsloth_generic_save_pretrained_merged(self, save_directory: Union[str, os.PathLike], tokenizer=..., save_method: str = ..., push_to_hub: bool = ..., token: Optional[Union[str, bool]] = ..., is_main_process: bool = ..., state_dict: Optional[dict] = ..., save_function: Callable = ..., max_shard_size: Union[int, str] = ..., safe_serialization: bool = ..., variant: Optional[str] = ..., save_peft_format: bool = ..., tags: List[str] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> None:
    """
    Same as .push_to_hub(...) except 4bit weights are auto
    converted to float16 with as few overhead as possible.

    Choose for `save_method` to be either:
    1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
    2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
    3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
    """
    ...

def unsloth_generic_push_to_hub_merged(self, repo_id: str, tokenizer=..., save_method: str = ..., use_temp_dir: Optional[bool] = ..., commit_message: Optional[str] = ..., private: Optional[bool] = ..., token: Union[bool, str, None] = ..., max_shard_size: Union[int, str, None] = ..., create_pr: bool = ..., safe_serialization: bool = ..., revision: str = ..., commit_description: str = ..., tags: Optional[List[str]] = ..., temporary_location: str = ..., maximum_memory_usage: float = ...): # -> None:
    """
    Same as .push_to_hub(...) except 4bit weights are auto
    converted to float16 with as few overhead as possible.

    Choose for `save_method` to be either:
    1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
    2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
    3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
    """
    ...

def unsloth_save_pretrained_torchao(self, save_directory: Union[str, os.PathLike], tokenizer=..., torchao_config=..., push_to_hub: bool = ..., token: Optional[Union[str, bool]] = ...): # -> None:
    """Quantizes the model with torchao and saves a torchao quantized checkpoint

    Args
      `save_directory`: local folder path or huggingface hub ID when `push_to_hub` is set to True, e.g. `my_model`
      `torchao_config` (TorchAOBaseConfig): configuration for torchao quantization, full list: https://docs.pytorch.org/ao/main/api_ref_quantization.html#inference-apis-for-quantize
      `push_to_hub` (bool): whether to push the checkpoint to huggingface hub or save locally
    """
    ...

def not_implemented_save(*args, **kwargs):
    ...

def patch_saving_functions(model, vision=...):
    ...

